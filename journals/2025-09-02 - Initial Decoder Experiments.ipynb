{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "975652ff"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55nv-QXmAhQ1"
      },
      "source": [
        "# Initial Decoder Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohD215ZwAkXe"
      },
      "source": [
        "(Author: Chris McCormick)\n",
        "\n",
        "I ran Decoder pre-training + fine-tuning experiments at a similar scale to what we've tried with the `SubspaceEncoder`.\n",
        "\n",
        "For these:\n",
        "* I did more thorough sweeps of the subspace sizes.\n",
        "* Instead of implementing a custom class like the `SubspaceEncoder`, I patched the huggingface implementation in `transformers.DeepseekV3`\n",
        "\n",
        "The notes below document the experiment and results, but also some general project updates and insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySzMAKKKonPF"
      },
      "source": [
        "## S1. General"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff57b05c"
      },
      "source": [
        "### 1.1. Performance Improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TufG_CEQ3NHL"
      },
      "source": [
        "The below changes would probably make sense to apply to the Encoder setup as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fYJKW7vC8w"
      },
      "source": [
        "**torch.compile**\n",
        "\n",
        "I found that `torch.compile` did give some easy performance gains.\n",
        "\n",
        "In the JSON configuration files:\n",
        "\n",
        "```python\n",
        "  \"pre_train\": {\n",
        "    ...\n",
        "    \"bf16\": true,\n",
        "    \"fp16\": false,\n",
        "    \"torch_compile\": true,\n",
        "    \"torch_compile_backend\": \"inductor\",\n",
        "    \"torch_compile_mode\": \"default\"\n",
        "  },\n",
        "```\n",
        "\n",
        "I didn't record the difference, but I think it was in the neighborhood of 15-25% faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XTD1UnTB2sX"
      },
      "source": [
        "\n",
        "**bf16**\n",
        "\n",
        "I also added support for bf16, since it's generally recommended when available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ3fk6Kxz3rL"
      },
      "source": [
        "**Example Packing**\n",
        "\n",
        "The basic training approach can result in training on a large number of padding tokens, especially as the sequence length gets longer. wikitext103 contains full articles, but many can be stubs.\n",
        "\n",
        "For the final training runs at sequence length 1,024, I switched to using concatenated samples. This definitely seemed to help on perplexity and benchmark accuracy--I think the models effectively got to see more (real) tokens.\n",
        "\n",
        "Here's what the code looks in `subspace_decoder\\scripts\\train.py`. I let GPT 5 write this and haven't done a fine-grained review, but here's what's generally going on.\n",
        "\n",
        "First, you do a raw tokenization (no padding, no BOS or EOS):\n",
        "\n",
        "```python\n",
        "\n",
        "    block_size = ptrain_cfg[\"max_seq_length\"]\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    \n",
        "    # 1) Tokenize without truncation/padding\n",
        "    def tokenize_function(examples):\n",
        "        # add_special_tokens=False keeps things raw; we’ll insert EOS between docs\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=8,\n",
        "        remove_columns=dataset[\"train\"].column_names,  # drop raw \"text\"\n",
        "    )\n",
        "\n",
        "```\n",
        "\n",
        "Then,\n",
        "\n",
        "1. Take a batch of samples and concatenate them all, putting EOS tokens in between samples.\n",
        "2. Break it back apart into samples that are max_seq_len (i.e., 1024).\n",
        "3. Drop the final block so that every sample is full length.\n",
        "\n",
        "```python\n",
        "\n",
        "    # 2) Group into contiguous 1024-token blocks (concat + chunk)\n",
        "    def group_texts(examples):\n",
        "        # Flatten and insert EOS between documents to avoid cross-article bleed\n",
        "        input_ids = []\n",
        "        for ids in examples[\"input_ids\"]:\n",
        "            if len(ids) > 0:\n",
        "                input_ids.extend(ids)\n",
        "            # add an EOS fencepost between docs\n",
        "            input_ids.append(eos_id)\n",
        "    \n",
        "        # Drop the trailing partial block so every example is full length\n",
        "        total_length = (len(input_ids) // block_size) * block_size\n",
        "        input_ids = input_ids[:total_length]\n",
        "    \n",
        "        # Split into equal blocks\n",
        "        result_input_ids = [input_ids[i:i + block_size] for i in range(0, total_length, block_size)]\n",
        "        # Labels are next-token targets; Trainer/model will do the shift\n",
        "        return {\n",
        "            \"input_ids\": result_input_ids,\n",
        "            \"labels\": [ids.copy() for ids in result_input_ids],\n",
        "            # Optional attention masks (all ones because no padding)\n",
        "            \"attention_mask\": [[1] * block_size for _ in result_input_ids],\n",
        "        }\n",
        "       \n",
        "    # Concatenate + chunk\n",
        "    tokenized = tokenized.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        num_proc=8,\n",
        "    )\n",
        "    \n",
        "    # Use a simple collator; we already created labels and have no pads\n",
        "    from transformers import default_data_collator\n",
        "    data_collator = default_data_collator\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm7RIp_Xs402"
      },
      "source": [
        "### 1.2. Talking to DeepSeek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dGVauMhs7D2"
      },
      "source": [
        "I reached out to Huazuo Gao at DeepSeek, one of the main contributors to MLA.\n",
        "\n",
        "He confirmed that they're working on the output projection themselves (to reduce flops), but haven't found an acceptable solution yet.\n",
        "\n",
        "He connected me with Wangding Zeng, another author at DeepSeek, who's been doing the experiments. I've reached out to him as well to see if he's willing to share any insights.\n",
        "\n",
        "**Implications**\n",
        "\n",
        "* That was cool confirmation that this is worth investigating.\n",
        "* It also suggests we might be able to publish something if we take an easier angle on it (e.g., maybe more efficiency or analysis centered).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGFLVTcaqydn"
      },
      "source": [
        "### 1.3. Patching vs. Re-Implementing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFiOTGgyq2cb"
      },
      "source": [
        "Initially, I pulled in the code for the DeepSeekV3Attention class and added the output decomposition, with the attention of patching the entire attention block. This code is still present in `subspace_decoder/layers/deepseek_mla_o.py`.\n",
        "\n",
        "I learned, though, that you can use a DeepSeekV3 instance as-is, and simply patch the `o_proj` module (it's an `nn.Linear`) with a `nn.Sequential` which performs the two steps.\n",
        "\n",
        "i.e., you can do something like:\n",
        "\n",
        "```python\n",
        "attn.o_proj = nn.Sequential(\n",
        "    nn.Linear(in_features,  o_latent_dim,  bias=False),  # W^OA\n",
        "    nn.Linear(o_latent_dim, out_features, bias=bias),    # W^OB\n",
        ")\n",
        "```\n",
        "\n",
        "This means that it's actually pretty trivial to apply the output decomposition to any existing model implementation--you don't need to re-implement the attention block.\n",
        "\n",
        "_However_--re-loading from disk is a little uglier. You load the base model from disk, patch `o_proj`, and then you have to manually load the output weights in from the safetensors file.\n",
        "\n",
        "The \"patch o_proj\" approach is what I took for these experiments. The functions for doing this are in `subspace_decoder/layers/patch_o_proj.py`.\n",
        "\n",
        "It got more complex because I experimented with a number of different normalization strategies (discussed more further down).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1965e9"
      },
      "source": [
        "### 1.4. RoPE and NoPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "665bcb62"
      },
      "source": [
        "For our head size of 32, I had intended to do 16 RoPE dims and 16 NoPE dims, but there's a HuggingFace utility function involved with the rotary embeddings that throws an error.\n",
        "\n",
        "(I remember I encountered this and resolved it in the custom Encoder implementation. I think it was that the function tries to infer the RoPE head size from d_model // num_heads?)\n",
        "\n",
        "Instead, I used 32 RoPE dims and 0 NoPE dims. Here's what that means:\n",
        "\n",
        "1. The **key-value latent** space is only being used for the **value** heads.\n",
        "2. DeepSeekV3 uses a single key head for RoPE. Because I set NoPE to 0, it means that the model is currently only using this **single key head** (but does still have 8 query heads).\n",
        "   * So we have 8 query heads, 1 key head, and 8 value heads.\n",
        "\n",
        "I'd like to fix this and go with 16 RoPE dims and 16 NoPE dims. It might be trickier here than in the Encoder, since here we're just patching the existing DeepSeekV3 classes rather than overwriting any of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c7BsPf-cEOj"
      },
      "source": [
        "### 1.5. MoE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8ZbHxRjcFg8"
      },
      "source": [
        "I had intended to just use all dense layers, just in case MoE training had issues.\n",
        "\n",
        "Somehow that config change got lost, and so the models were trained with 1 initial dense layer and MoE layers instead.\n",
        "\n",
        "It seemed to work fine, though? It might be good to assess the router health in the final models to make sure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09db241b"
      },
      "source": [
        "## S2. Benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCgZpiyzolwh"
      },
      "source": [
        "### 2.1. Setup & Runtimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2efc77a"
      },
      "source": [
        "I ran everything on a lambda instance with a GH200 with 96 GB. Pretty sweet!\n",
        "\n",
        "Most of my experiments were sweeps over the latent space sizes and done at a sequence length of 128. Those pretraining runs took ~35 minutes each.\n",
        "\n",
        "I did two final training runs (one for MLA, one for MLA-o) with the best settings and a sequence length of 1024. Those took about 5.5 hours each. Still not too bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "827b2be4"
      },
      "source": [
        "### 2.2. Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56b68dd8"
      },
      "source": [
        "**Accuracy**\n",
        "\n",
        "* Used perplexity on wikitext103 and accuracy on SST-2.\n",
        "* For SST-2, I prompted the model with the task, gave a couple examples, and then looked at the logits for the predicted token.\n",
        "* Somewhat surprisingly, the decoder actually outperformed the encoder on SST-2. My guess would be it's related to the Mixture of Experts?\n",
        "* Overall, MLA-o underperforms MLA, similar to what we saw with the Encoder.\n",
        "\n",
        "**Throughput**\n",
        "* Still not seeing a speed benefit from decomposition, even at sequence length 1024.\n",
        "* Next step might be to use 12 heads instead of 8, since head count and size matter for performance benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8UCTsAZbiS9"
      },
      "source": [
        "**Weights and Biases**\n",
        "\n",
        "Decoder Pre-Training workspace [here](https://wandb.ai/chrismccormick/decoder-pretrain-wiki103) and sst-2 [here](https://wandb.ai/chrismccormick/decoder-finetune-sst2), but note that they're raw / not organized for presenting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddplt7dWq-Dr"
      },
      "source": [
        "### 2.3. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWlLjMORrAJA"
      },
      "source": [
        "The rows compare MLA vs. MLA-o, and the columns compare them at a sequence length of 128 vs. 1024.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knI5xv0n-scH"
      },
      "source": [
        "**Subspace Sizes**\n",
        "\n",
        "These are the sizes that performed the best.\n",
        "\n",
        "```\n",
        " Query   96\n",
        "    KV   64\n",
        "Output   96  (for MLA-o)\n",
        "```\n",
        "\n",
        "Some of the larger configurations did worse on perplexity but did do slightly better on SST-2. Since the difference wasn't large and we're going for throughput gains, I stuck with the above sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyRXGhzD-vTv"
      },
      "source": [
        "**Comparing Sequence Lengths**\n",
        "\n",
        "I included results both at length 128 and at 1024 to see how MLA and MLA-o compare to eachother in the short vs. \"long\" context setting.\n",
        "\n",
        "However, the length 128 results shouldn't be compared to the length 1024 results, because only the latter used \"example packing\".\n",
        "\n",
        "In other words, compare the rows below, don't compare across the columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEyT03Zo-t4n"
      },
      "source": [
        "**Accuracy and Perplexity**\n",
        "\n",
        "\n",
        "\n",
        "SST-2 Accuracy (higher is better)\n",
        "```\n",
        "         1,024                  128\n",
        "         -----                 -----\n",
        "MLA-o:   86.24                 85.78           \n",
        "  MLA:   87.96  +1.72          87.04  +1.26\n",
        "```\n",
        "\n",
        "\n",
        "Wikitext103 Perplexity (lower is better)\n",
        "\n",
        "```\n",
        "         1,024                  128\n",
        "         -----                 -----\n",
        "MLA-o:   29.33                 40.94\n",
        "  MLA:   28.89   -0.44         39.21   -1.73\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWlPGC99-eL_"
      },
      "source": [
        "\n",
        "_Model Size & Training Time_\n",
        "```\n",
        "                                   Time to Pre-Train\n",
        "         Params                   1,024          128         \n",
        "         ------                  ------          ---\n",
        "MLA-o:   16.17M                  5h 31m          36m\n",
        "  MLA:   16.26M   + ~90K         5h 31m          36m\n",
        "```\n",
        "\n",
        "I was expecting that MLA-o would start to be faster at the longer sequence length based on some earlier throughput benchmarks I did, but there's almost no speed difference so far.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mntV3ios9ubg"
      },
      "source": [
        "**Training Curves**\n",
        "\n",
        "That grad norm is interesting, I wonder what it means that it was twice the magnitude?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RbR7gRdvKku"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1vgZAtuFv-kCyeBizMHGu6UyVtSUBRdKh' alt='Training curve comparisons' width='900' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSs9H6wswWrd"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1EMn613Ufj_YDzC1kkd2x9TdaqfPwr9WG' alt='Comparison of pretraining perplexity and eval throughput' width='900' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04289e6"
      },
      "source": [
        "## S3. Normalization Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58e998b3"
      },
      "source": [
        "(Some rough notes that GPT captured for me, I'll have to revisit this)\n",
        "\n",
        "* Tried several normalization strategies:\n",
        "\n",
        "  1. **Per-head RMSNorm with independent gain weights** → highest performing variant, but slowed things down.\n",
        "  2. **RMSNorm after summing the heads** → nearly as good, no slowdown.\n",
        "  3. **Per-head RMSNorm with a shared gain vector** → not as good as (1), also slow.\n",
        "* For now I’m sticking with the **post-sum RMSNorm** as the best trade-off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2db8aa07"
      },
      "source": [
        "## S4. Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQJKp-8dFVpa"
      },
      "source": [
        "**Throughput Experiments**\n",
        "\n",
        "I'd like to try scaling the model further to find the point where MLA-o is faster than MLA.\n",
        "\n",
        "We don't need to fully pre-train a model to do this, though.\n",
        "\n",
        "* For **inference** throughput, we could just leave the model randomly initialized and send text through it.\n",
        "* For **training** throughput, we could train for just a small number of steps (e.g., 1000?).\n",
        "\n",
        "Increasing the number of heads and their size is the next thing I'd try.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dV46QD0D7np"
      },
      "source": [
        "**Analyzing / Evaluating the New Models**\n",
        "\n",
        "(1) We could try testing these two new long-sequence models on more benchmarks beyond SST-2. 1,024 tokens is enough for many of them. For example, we could do imdb movie reviews instead.\n",
        "\n",
        "(2) We could try analyzing / characterizing their behavior.\n",
        "* We now have two models trained identically, one with the output latent and one without. Maybe there are some interesting ways to compare them?\n",
        "* The normalization strategy for the output is an interesting challenge--it could be interesting to compare magnitudes of the attention output between the two. Is MLA-o producing larger outputs?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42790f03"
      },
      "source": [
        "**Pre-Training Experiments**\n",
        "\n",
        "Here are some thoughts on potential issues to resolve and new things to try.\n",
        "\n",
        "(1) Address the **RoPE issue** so we can run with split dims (16 RoPE / 16 non-RoPE) as originally intended.\n",
        "* I think the options to weigh are:\n",
        "    1. Continue trying to surgically patch the DeepseekV3 code.\n",
        "    2. Do a more heavy-handed patch by overwriting the Attention class entirely.\n",
        "    3. Make a Decoder variant of the SubspaceEncoder.\n",
        "    \n",
        "(2) Review training curves.\n",
        "* What's going on with the high grad norm for MLA-o? Is that a problem?\n",
        "* Does the data tell us whether we could have stopped earlier, or whether it might be beneficial to train on more tokens?\n",
        "\n",
        "(3) Compare impact of Output subspace to impact of Query subspace\n",
        "* To what degree does adding a Query subspace hurt performance?\n",
        "   * It would be interesting to see if the loss in accuracy and change in throughput are similar or not.\n",
        "* I noticed that in DeepSeek-V2-Lite, they don't use a latent space for the Query heads. Perhaps these decompositions are more harmful / less beneficial at smaller scales?\n",
        "\n",
        "(4) Review example packing.\n",
        "* This is a tricky topic that I'd like to understand better.\n",
        "* It sounds great (and seemed to work!), but how it affects the model is still a little unclear to me.\n",
        "* Some more specific questions for our case:\n",
        "    * Is it ok to drop trailing blocks? How many tokens are being lost?\n",
        "    * Do we need to define a separate padding token?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIiGTEI-GtUh"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYZL1cH9j9A"
      },
      "source": [
        "# Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00fHntZLNlCS"
      },
      "source": [
        "## Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lvET23HGryq"
      },
      "source": [
        "Some lessons learned for me on the tooling and methodology side.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHKM9bmjKW4d"
      },
      "source": [
        "\n",
        "**Cursor**\n",
        "\n",
        "I finally took the plunge and signed up for Cursor.\n",
        "\n",
        "_Tab completions_\n",
        "\n",
        "My favorite part so far are the tab completions.\n",
        "\n",
        "I've appreciated the completions in Google Colab, but they're limited to continuing what you're currently writing. You have to be at the end of a line, and it will suggest what to write next.\n",
        "\n",
        "Cursor broadens this to making suggestions for the surrounding code, and will also make suggestions for the entire file that you can quickly tab through and accept. It's incredibly helpful.\n",
        "\n",
        "_Agent mode_\n",
        "\n",
        "I've used OpenAI's Codex. What I appreciate in Cursor is how much easier it is to modify and accept the changes.\n",
        "\n",
        "I've heard great things about Claude 4, but so far have felt disappointed.\n",
        "\n",
        "I still find that I get the most intelligent / useful results by copying code into a GPT 5 conversation. Maybe it's because I do the work of identifying the correct context to give it, whereas the agent is filling up the context trying to make sense of the codebase on its own.\n",
        "\n",
        "Of course, copying the code back out from a conversation and merging it in is such an obnoxious process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqEjZdN8L70L"
      },
      "source": [
        "**Lambda**\n",
        "\n",
        "I can probably improve my workflow significantly–I was doing a lot of copy-pasting between Cursor, GPT, and the Lambda instance.\n",
        "\n",
        "I bet there's a way to run the Jupyter Notebook in Cursor, and have it hooked in to a Python kernel running on the Lambda instance.\n",
        "\n",
        "That still leaves file management. Can the code maybe live on the Lambda instance, but still be editable in Cursor?\n",
        "\n",
        "Also, getting a good diff tool would be big. (Done! It's already there--right click the files and use the \"select for compare\" options).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W02efjswNayG"
      },
      "source": [
        "**Google Drive**\n",
        "\n",
        "I wanted to continue using Google Drive for storing checkpoints. Getting that working was a pain in the butt, but I got there.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-sPzEJjNto-"
      },
      "source": [
        "**Git**\n",
        "\n",
        "I'm still a noob at Git. I've really only ever done a simple `pull` / `commit` / `push` workflow.\n",
        "\n",
        "For the experimenting that I was doing, creating a branch to work in, and then merging that branch back (after cleaning up / erasing the commit history) would be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tscl1c8jNsvc"
      },
      "source": [
        "**Weights and Biases**\n",
        "\n",
        "wandb seems like it could be so much more useful if I understood it better.\n",
        "\n",
        "Some things I want to figure out:\n",
        "\n",
        "(1) Tables!\n",
        "* These are what I really want for looking at results...\n",
        "    * I went and dug around--I think a good initial answer is the \"runs\" tab. It's pretty much the table view I'm looking for.\n",
        "    * Still don't know why it's so hard to define custom tables to include as panels, but oh well.\n",
        "\n",
        "(2) Customizing the Dashboard\n",
        "* This ought to be simple--creating a section that has just the panels I'm most interested in.\n",
        "    * Maybe it is, but I feel like I've had trouble with it before.\n",
        "* Even simple zooming is something I haven't figured out yet!\n",
        "\n",
        "(3) Organizing Runs\n",
        "* I always end up with a lot of runs, and have to sift through them to show and hide different ones to make comparisons.\n",
        "* I think the solution may be to set up the \"runs\" view properly (to show just the columns I'm interested in), and then use it to handle selecting what I want to show or hide.\n",
        "* Being able to backfill values would be really helpful--is that possible?\n",
        "    * Doing it from code might be a workaround, or by using tags.\n",
        "\n",
        "(4) Auto-delete cancelled runs\n",
        "* While debugging or solving OOM issues, I'll often end up with a bunch of spurious runs in the project. I can probably put something in the code which will automatically delete the run under certain conditions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c8dbaa"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
