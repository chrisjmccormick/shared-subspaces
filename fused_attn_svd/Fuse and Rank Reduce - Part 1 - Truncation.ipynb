{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKTih91CsQvU1F7/aB5TnI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjmccormick/shared-subspaces/blob/main/fused_attn_svd/Fuse%20and%20Rank%20Reduce%20-%20Part%201%20-%20Truncation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avSgZebOUiIx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEkn7TwmPuaJ"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMcMuxOAPw4e"
      },
      "source": [
        "# Fuse and Rank Reduce (FaRR)\n",
        "\n",
        "_Finding Hidden Low-Rank Structure in Attention_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojP9lZ6qP4bU"
      },
      "source": [
        "Most SVD-based compression methods start by taking **one matrix** and breaking it into **two**.\n",
        "That makes each piece smaller — but you’ve now added an extra multiply.\n",
        "In fact, the parameter count *goes up* until you can remove enough singular components to offset the decomposition.\n",
        "\n",
        "Fuse and Rank Reduce (FaRR) works differently.\n",
        "Instead of introducing an extra step, it starts from **two matrices that are already multiplied together in the model** and recomposes them into a single fused matrix.\n",
        "When you truncate this fused matrix and split it back apart, you keep the *same* number of multiplies as before — but with fewer parameters.\n",
        "\n",
        "There are **two important caveats**, in addition to the standard quality–vs.–compression trade-off:\n",
        "\n",
        "1. **It’s not always there** — the kind of low-rank structure you can exploit this way is the exception, not the norm.\n",
        "2. **It can be hard to leverage without breaking parallelism** — attention heads are shaped for batch-friendly execution, and mismatched head sizes can erase your gains.\n",
        "\n",
        "The method applies anywhere in a model where you have a **linear map** *(i.e., two matrices multiplied together with no nonlinearity in between)*.\n",
        "In transformers, the prime examples are:\n",
        "\n",
        "* **Value–Output** in the attention write path.\n",
        "* **Query–Key** in the attention score path.\n",
        "\n",
        "There are two opportunities to leverage FaRR that we'll explore:\n",
        "\n",
        "1. **Rank reduction** — Fuse two matrices, measure the effective rank, truncate, and split them back into smaller matrices.\n",
        "   In Part 1, we’ll show exactly how to do this and explore where the opportunity exists in the Kimi-K2 model.\n",
        "2. **Shared subspace identification** — Fuse matrices to reveal where multiple heads use overlapping latent spaces.\n",
        "   In Part 2, we’ll dig into *why* fused matrices sometimes have much lower rank, and use that insight to locate shared subspaces for even more parameter savings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eabc857"
      },
      "source": [
        "## S1. SVD Review & Notation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1hQ2uEN6jSE"
      },
      "source": [
        "Before we start fusing matrices, let’s review what Singular Value Decomposition (SVD) actually *does* — in an intuitive, vector-level way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GON1kav6fF-"
      },
      "source": [
        "### 1.1. Vector Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3UwOjIrd1Re"
      },
      "source": [
        "\n",
        "\n",
        "When applied to a weight matrix \\$W\\$, SVD finds a set of vector pairs \\$(u_i, v_i)\\$, which, along with a scalar value \\$\\sigma_i\\$, fully capture the behavior of the original matrix:\n",
        "\n",
        "$$\n",
        "W = \\sum_{i} \\sigma_i \\, u_i \\, v_i^\\top\n",
        "$$\n",
        "\n",
        "* \\$u_i\\$ and \\$v_i\\$ are called **singular vectors**.\n",
        "* \\$\\sigma_i\\$ is called their **singular value**.\n",
        "\n",
        "The vectors are **unit length**, meaning they purely capture **direction**.\n",
        "They are sorted by their \\$\\sigma\\$ value, which tells us how relevant they are to the matrix’s behavior and how important they will be in preserving its full functionality.\n",
        "\n",
        "For example:\n",
        "\n",
        "* If we take the direction \\$v_0\\$ and project it through \\$W\\$, we get:\n",
        "\n",
        "$$\n",
        "v_0 W = \\sigma_0 u_0\n",
        "$$\n",
        "\n",
        "Here \\$v_0\\$ is the **input direction** that produces the strongest possible response from \\$W\\$ (given a normalized input).\n",
        "SVD separates that response into its **magnitude** (\\$\\sigma_0\\$) and its **direction** (\\$u_0\\$).\n",
        "\n",
        "In matrices with low effective rank, the last singular vector, $v_d$ will have $\\sigma_d$ near zero, meaning that direction produces almost no response from $W$.\n",
        "\n",
        "Those trailing directions — with very small $\\sigma$ — are the ones we can potentially drop with minimal impact to the matrix’s behavior.\n",
        "\n",
        "_Unit Length and Orthogonal_\n",
        "\n",
        "The vectors $v_i$ all have unit length and each one is orthogonal to every other. Their matrix is \"orthonormal\".\n",
        "\n",
        "The same is true within the collection of $u_i$ vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAZbVIpuP38"
      },
      "source": [
        "### 1.2. Matrix Formulation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpq8OVkKAvuB"
      },
      "source": [
        "The terminology used in the formal definition of SVD is problematic in the context of Transformers because it explicitly places the input on the right hand side of the equation (e.g., $Ax$) whereas the definition of Self-Attention places it on the left (e.g., $xW^Q$).\n",
        "\n",
        "To bring the definition inline with out context, we will:\n",
        "\n",
        "1. Define a \"variant\" of SVD which accepts and returns standard Attention variable names and shapes.\n",
        "2. Adopt the LoRA-style convention of \"$A$\" for the input matrix and \"$B$\" for the output matrix.\n",
        "3. Define the singular values as a vector $\\sigma$ instead of a diagonal matrix $\\Sigma$. This aligns with the implementation in deep learning libraries such as PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**SVD Applied to Attention Heads**\n",
        "\n",
        "Let $\\text{SVD}_\\text{QKV}(\\cdot)$ define a function which decomposes an individual query, key, or value head. Using a query head to illustrate,\n",
        "\n",
        "$$\n",
        "\\text{SVD}_\\text{QKV}(W^Q) = W^{QA} \\, \\sigma^Q \\, W^{QB}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $W^{QA} \\in \\mathbb{R}^{d_\\text{model} \\times r} \\quad $ contains orthonormal column vectors,\n",
        "* $\\sigma^Q \\in \\mathbb{R}^{r} \\quad \\quad \\quad $ contains their singular values, and\n",
        "* $W^{QB} \\in \\mathbb{R}^{r \\times d_\\text{head}} \\quad $ contains orthonormal row vectors.\n"
      ],
      "metadata": {
        "id": "HWNL_MWp9XaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Element-wise Multiplication**\n",
        "\n",
        "In our notation, the multiplication operation with $\\sigma$ is inferred by its placement. For example,\n",
        "* When multiplied with the matrix to its left, $(W^{QA} \\sigma^Q)$, the singular values are applied element-wise to the columns.\n",
        "* When multiplied with the matrix to its right, $(\\sigma^Q W^{QB})$, the singular values are applied element-wise to the rows.\n"
      ],
      "metadata": {
        "id": "nupR4hyI44h-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**In PyTorch**\n",
        "\n",
        "All of this aligns well with how we'll express the operation in our PyTorch examples:\n",
        "\n",
        "```python\n",
        "# ======== Variable Shapes ========\n",
        "#       W_q  (d_model, d_head)\n",
        "#      W_qa  (d_model, r     )  \n",
        "#      W_qb  (      r, d_head)\n",
        "#   sigma_q  (r,)\n",
        "#         x  (d_model,)\n",
        "# Where, initially, r = d_head.\n",
        "W_qa, sigma_q, W_qb = torch.linalg.svd(W_q)\n",
        "\n",
        "# Examples of how to project an input vector `x` into query space:\n",
        "\n",
        "# 1. Original matrix\n",
        "q1 = x @ W_q\n",
        "\n",
        "# 2. Apply sigma directly to x\n",
        "q2 = (x * sigma_q) @ W_qa @ W_qb\n",
        "\n",
        "# 3. Fold σ into the columns of W_qa\n",
        "q3 = x @ (W_qa * sigma_q.unsqueeze(0)) @ W_qb\n",
        "\n",
        "# 4. Fold σ into the rows of W_qb\n",
        "q4 = x @ W_qa @ (W_qb * sigma_q.unsqueeze(1))\n",
        "```\n"
      ],
      "metadata": {
        "id": "WJ4f8PPw45oX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYafnDKi9qKQ"
      },
      "source": [
        "### 1.3. SVD Truncation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2C49pvY9qKR"
      },
      "source": [
        "As an example, below is the decomposition of Query head number 50 from layer 13 of Moonshot's Kimi-K2--many of the Query heads in this layer have low effective rank.\n",
        "\n",
        "Kimi-K2 uses a query latent space of length 1,536, and a head size of 128.\n",
        "\n",
        "The below heatmaps show all of the floating point values in the respective matrices. After decomposing $W^Q_{50}$, we have $W^{QA}$, $\\sigma^Q$, and $W^{QB}$.\n",
        "\n",
        "Below are the contents of ${W^{QA}}^\\top$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AplHJw_QEiS"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1kTHmXdBKyPfOMsj0-9LUjtcgaGzDiSga' alt='Heatmap showing the full contents of the Wqa portion of the decomposition of layer 13 query head 50' width='900' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8e-e_1yepfM"
      },
      "source": [
        "> Aside: The striation pattern was surprising to see; it's not an effect of the decomposition--the original head seems to have the same pattern. Decomposition only normalizes the head dimensions of this matrix (the 128 rows in this plot are normalized, but not the columns). It's something we can come back to in part 2 when we look at the concept of shared subspaces.\n",
        "\n",
        "Below is the other half of the decomposition, $W^{QB}$, with no perceivable patterns. The row vectors in this matrix are sorted by the magnitude of sigma, meaning we'll remove rows from the bottom if we do any truncating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrD5sjouUl8C"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1i6LRvgyEEykmko3-rkg7kIaSgr-VKFO4' alt='Heatmap showing the contents of the 128 x 128 W_qb matrix of Kimi-K2 Layer 13 query head 50. Looks like noise, no pattern.' width='600' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3t_dU0Ngc0h"
      },
      "source": [
        "\n",
        "Next, we can visualize the singular values for this query head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9-RVYYzTHnT"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1jmG0N4gCPL93XRIRlkxP_1orELFPIKt8' alt='Line plot showing the singular values of Kimi-K2 Layer 13 query head 50. This is a very low-rank head with the elbow around 64.' width='600' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy6IPJIMfH-p"
      },
      "source": [
        "Each sigma value corresponds to a column in $W^{QA}$ and a row in $W^{QB}$.\n",
        "\n",
        "The 128 columns of $W^{QA}$ can be viewed as directions in the residual stream from which this query head will read.\n",
        "\n",
        "The 128 rows of $W^{QB}$ are directions in a 128-dimensional space along which this head will write those values.\n",
        "\n",
        "In between those two, however, are these sigma values, which capture how much the head will amplify or suppress/ignore the respective read/write pair.\n",
        "\n",
        "This plot is showing us that this query head is largely ignoring roughly half of its directions.\n",
        "\n",
        "We can visualize the effect of these sigma values by multiplying them with their corresponding row of $W^{QB}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aamZ7TJbL29"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1zr1vjTQXjVOrnZXMC3dpabxRrAhoJRyQ' alt='Heatmap showing the contents of the 128 x 128 Wqb matrix after scaling with sigma, from Kimi-K2 Layer 13 query head 50. This low rank head becomes largely white.' width='600' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXJLjuWGirQ1"
      },
      "source": [
        "The contrast has increased for the top rows, meaning they've been amplified, and many of the lower vectors are nearly 0.\n",
        "\n",
        "With this folding, $W^{QA}$ may read information along all 128 of its residual stream directions, but half those values will be projected onto vectors which have nearly zero magnitude, erasing what's there.\n",
        "\n",
        "This is what SVD Truncation takes advantage of--we can trim off these directions which seem to be contributing very little.\n",
        "\n",
        "A common heuristic for choosing how many vectors to remove is to measure the \"cummulative energy\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_GJPVHC9qKR"
      },
      "source": [
        "### 1.4. Effective Rank via Cumulative Energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QuUtl2xn39F"
      },
      "source": [
        "A common method for quantifying the \"effective rank\" of a matrix (and deciding how many dimensions to drop / keep), is to keep the smallest $r$ such that the retained singular values account for some fraction of the total “energy”:\n",
        "\n",
        "$$\n",
        "\\frac{\\sum_{j=1}^r \\sigma_j^2}{\\sum_{j=1}^{d_v} \\sigma_j^2} \\ge \\tau\n",
        "$$\n",
        "\n",
        "with $\\tau$ set to something like $0.99$ or $0.999$ (99% or 99.9%).\n",
        "\n",
        "Here is that metric applied to query head 50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0qkI02MnFDC"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/12eOZ8tFZOBTFqz12-niKTYaZrzeKRPN8' alt='Plot showing the overlay of this query head cumulative energy and singular values along with the locations of the 99 and 99.9 energy points' width='800' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnfWAnm1n67N"
      },
      "source": [
        "Using the 99.9% threshold, the effective rank of this query head is 66.\n",
        "\n",
        "The below demonstrates how we calculate the effective rank of a query head, where you would replace `W_q` with actual weight values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the cumulative energy function\n",
        "def cumulative_energy(sigma, tau):\n",
        "    sigma_e = sigma ** 2 # 'Energy' from each sigma\n",
        "    total_e = np.sum(sigma_e) # Total energy\n",
        "    cumulative_e = np.cumsum(sigma_e) # Cumulative energy by sigma\n",
        "    sigma_ratios = cumulative_e / total_e # Fraction of total\n",
        "    return np.argmax(sigma_ratios >= tau) + 1 # Index of sigma which exceeds tau\n",
        "\n",
        "# Step 2: Retrieve the query head\n",
        "W_q = torch.randn(d_model, d_head)\n",
        "\n",
        "# Step 3: Decompose the query head\n",
        "# Shapes\n",
        "#       W_q  (d_model, d_head)\n",
        "#      W_qa  (d_model, r     )\n",
        "#      W_qb  (      r, d_head)\n",
        "#   sigma_q  (r,)\n",
        "# W_qb is a square matrix with shape (d_head, d_head). The use of `r` helps\n",
        "# clarify the orientation of the singular vectors (they are the rows) and\n",
        "# indicates the dimension along which we'll truncate.\n",
        "W_qa, sigma_q, W_qb = torch.linalg.svd(W_q)\n",
        "\n",
        "# Step 4: Get the effective rank of this query head.\n",
        "eff_rank_q = cumulative_energy(sigma_q, 0.999)"
      ],
      "metadata": {
        "id": "x9vGTwiPAbfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1_QujBMBbF"
      },
      "source": [
        "**Note: Cumulative energy is a heuristic, not a guarantee**\n",
        "\n",
        "While this is a useful shorthand for “most of the matrix’s action is in these directions,” it’s not a performance guarantee.\n",
        "\n",
        "* Singular value energy is an *average-case* measure over all possible inputs.\n",
        "* A rare-but-important direction could correspond to a small singular value but still be semantically critical.\n",
        "* Energy ignores **structure** in the singular vectors — losing a low-energy direction could disproportionately affect specific tasks.\n",
        "\n",
        "So while “99.9% energy retained” *sounds* like “99.9% performance retained,” the correlation is imperfect and needs to be validated empirically.\n",
        "\n",
        "The goal here is not to redefine SVD truncation’s trade-offs — that’s been studied extensively — but to **improve how we identify latent bottlenecks** so that truncation targets the right dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Query-Key Truncation Example"
      ],
      "metadata": {
        "id": "IndkpFFGGN7b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9oMMX7cEi6M"
      },
      "source": [
        "The query head we explored earlier has a key head partner with similarly low rank.\n",
        "\n",
        "Using the cumulative energy metric with a threshold of 99.9%, the Layer 13 Query Head 50 has an effective rank of 66, and the corresponding Key Head 50 has an effective rank of 72.\n",
        "\n",
        "The size of the two heads needs to match, so if we want to retain our threshold then the lowest we can truncate them is to the top 72 singular vectors, (which may not be enough to reduce the total number of parameters?)\n",
        "\n",
        "The below code assumes we've repeated the above calculations for the key head, and now we'll truncate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHaavG_rF2YL"
      },
      "outputs": [],
      "source": [
        "# Given the effective ranks of the query and key head, we can choose a rank\n",
        "# that we will truncate to, 'r', which retains 99.9% energy. To do this, we\n",
        "# need to pick the larger of the two. This gives r = 72.\n",
        "r = max(eff_rank_q, eff_rank_k)\n",
        "\n",
        "# Truncate by taking the first `r` columns or rows of the decomposed matrices.\n",
        "W_qar = W_qa[:, :r]\n",
        "W_qbr = W_qb[:r, :]\n",
        "\n",
        "W_kar = W_ka[:, :r]\n",
        "W_kbr = W_kb[:r, :]\n",
        "\n",
        "sigma_q = sigma_q[:r]\n",
        "sigma_k = sigma_k[:r]\n",
        "\n",
        "# Fold the sigma values into their respective 'A' matrix; start by turning them\n",
        "# into row vectors, then element-wise multiply with their respective columns.\n",
        "W_qar = W_qar * sigma_q.unsqueeze(0)\n",
        "W_kar = W_kar * sigma_k.unsqueeze(0)\n",
        "\n",
        "# Now query and key vectors are calculated via:\n",
        "q = x @ W_qar @ W_qbr\n",
        "k = x @ W_kar @ W_kbr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xogeISUHGBVp"
      },
      "source": [
        "We can calculate the parameter savings to determine if we've improved things.\n",
        "\n",
        "The head shapes in Kimi-K2 are unusual because the input to them is a latent vector rather than the token vector. The query heads are $(1536, 128)$ and the key heads are $(512, 128)$.\n",
        "\n",
        "  * **Original Parameters**:\n",
        "      * $W^Q$: $1536 \\times 128 = 196,608$\n",
        "      * $W^K$: $512 \\times 128 = 65,536$\n",
        "      * **Total: 262,144 (256K)**\n",
        "\n",
        "We've decomposed and truncated these to $(1536, 72) \\times (72, 128)$ and $(512, 72) \\times (72, 128)$.\n",
        "\n",
        "  * **New Parameters (Individual Truncation)**:\n",
        "      * $W^Q \\rightarrow W^{QA_r} \\times W^{QB_r}$: $(1536 \\times 72) + (72 \\times 128) = 110,592 + 9,216 = 119,808$\n",
        "      * $W^K \\rightarrow W^{KA_r} \\times W^{KB_r}$: $(512 \\times 72) + (72 \\times 128) = 36,864 + 9,216 = 46,080$\n",
        "      * **Total: 165,888 (162K)**\n",
        "\n",
        "This is a **36.7%** reduction in parameters for this head, but we now have four matrices instead of two, and we can do better.\n",
        "\n",
        "By fusing the heads together before applying SVD, we can reveal a much lower joint effective rank—just **41**—for their combined operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S2. Fusing Attention Heads\n"
      ],
      "metadata": {
        "id": "_G4PIwn7vfCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The key insight of FaRR is to analyze the rank of the *combined linear map* rather than its individual components. When two matrices are multiplied, the rank of the resulting matrix can be much lower than the rank of either original matrix, especially if their latent spaces are misaligned or one is a subspace of the other.\n"
      ],
      "metadata": {
        "id": "D84GjP13wUv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. How to Fuse\n"
      ],
      "metadata": {
        "id": "RHrSWCGrvccg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In multi-head attention, each head contains two projection matrices that form a low-rank linear map. For example, for the value and output matrices:\n",
        "\n",
        "  * The **Value projection** $W^V_i$ maps from the model's embedding space ($d_\\text{model}$) into a smaller per-head value space ($d_v$).\n",
        "  * The **Output projection** $W^O_i$ maps from that per-head value space back into the model's embedding space.\n",
        "\n",
        "These two matrices are usually implemented as part of larger concatenated matrices for all heads for GPU efficiency. But if you conceptually “break apart” the concatenation, each head's contribution to the residual stream is just:\n",
        "\n",
        "$$\\text{output}_i = (\\alpha_i V_i) W^O_i$$\n",
        "\n",
        "where $\\alpha_i$ are the attention weights and $V_i = X W^V_i$.\n",
        "\n",
        "Mechanistic interpretability researchers—notably in Anthropic’s *Transformer Circuits* framework—often work with the fused matrix, as it represents the head's complete information pathway:\n",
        "\n",
        "$$W^{VO}_i = W^V_i \\, W^O_i$$\n",
        "\n",
        "The same fusion idea applies to the query/key side, where the matrices combine to produce the attention scores:\n",
        "\n",
        "$$W^{QK}_i = W^Q_i \\, (W^K_i)^\\top$$\n",
        "\n",
        "In both cases, this “fused” view lets us apply SVD to study a head’s true rank and subspace alignment—revealing inefficiencies and redundancy that aren’t visible when looking at just one side.\n",
        "\n",
        "(A more detailed derivation of these fused matrices can be found in the appendix.)"
      ],
      "metadata": {
        "id": "YSPiDZcKwYlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**RoPE Caveat**\n",
        "\n",
        "Most modern models prevent this simple fusion of the Query-Key matrices because they apply Rotary Positional Embeddings (RoPE) in between the projections. We *can*, however, apply FaRR to models like Kimi-K2, which only apply RoPE to a fraction of the head's dimensions, allowing us to fuse the non-RoPE part. More on that topic in the appendix.\n"
      ],
      "metadata": {
        "id": "HF0VRML5xVs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Query-Key Example (Revisited with FaRR)\n"
      ],
      "metadata": {
        "id": "_tqt3pG8wq65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's return to our Kimi-K2 Layer 13 Head 50 example. Instead of decomposing $W^Q$ and $W^K$ separately, we first fuse them.\n",
        "\n",
        "```python\n",
        "# Step 1: Fuse the query and key heads\n",
        "# W_q shape: (1536, 128)\n",
        "# W_k shape: (512, 128)\n",
        "W_qk = W_q @ W_k.T  # Fused shape: (1536, 512)\n",
        "\n",
        "# Step 2: Decompose the fused matrix\n",
        "W_qka, sigma_qk, W_qkb = torch.linalg.svd(W_qk)\n",
        "# W_qka: (1536, 512)\n",
        "# sigma_qk: (512,)\n",
        "# W_qkb: (512, 512)\n",
        "\n",
        "# Step 3: Get the effective rank of the fused matrix\n",
        "eff_rank_fused = cumulative_energy(sigma_qk, 0.999)\n",
        "# For this head, eff_rank_fused = 41\n",
        "```\n",
        "\n",
        "The fused effective rank is only **41**, far lower than the individual effective ranks of 66 (Query) and 72 (Key).\n",
        "\n",
        "We can now decompose back into new query and key matrices, and truncate them both to this lower rank.\n",
        "\n",
        "First, applying SVD to the fused matrix and folding the singular values into one of the matrices (how/where we fold them does not matter), gives back new query and key projections:\n",
        "\n",
        "$\\text{SVD}_{QK}(W^{QK}) = W^{QKA} \\sigma^{QK} W^{QKB} = \\hat{W}^{Q} (\\hat{W}^{K})^\\top $\n",
        "\n",
        "These new matrices have the same shape and accuracy as the originals, but now they are each orthonormal and sorted.\n",
        "\n",
        "Truncating them to their top 41 vectors yields a query matrix of size $(1536, 41)$ and a key matrix of size $(512, 41)$.\n",
        "\n",
        "  * **New Parameters (FaRR)**:\n",
        "      * New $W^Q$: $1536 \\times 41 = 62,976$\n",
        "      * New $W^K$: $512 \\times 41 = 20,992$\n",
        "      * **Total: 83,968 (82K)**\n",
        "\n",
        "This is a **67.9%** reduction from the original 256K parameters, and a further **49.4%** reduction compared to the 162K parameters from individual truncation. We're now back to having just two matrices, but with far fewer parameters."
      ],
      "metadata": {
        "id": "pbwnWzzVxQMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. FaRR in Practice: Illustrative Layers\n"
      ],
      "metadata": {
        "id": "xHRdz_Q3__Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We've seen how FaRR can compress a single head. Now, let's examine how these low-rank opportunities manifest across a model. By analyzing different layers from DeepSeek-R1, we can identify two primary patterns where FaRR proves effective, as well as the more common case where it does not.\n",
        "\n",
        "**Case 1: The Bottleneck Effect**\n",
        "\n",
        "The rank of a fused matrix is mathematically constrained by the ranks of its constituent matrices:\n",
        "$$\\operatorname{rank}(W^{VO}) \\le \\min\\big(\\operatorname{rank}(W^V), \\operatorname{rank}(W^O)\\big)$$\n",
        "This means that if one matrix in a pair already has a low effective rank, it creates a bottleneck that the fused matrix cannot exceed. This is clearly visible in Layer 8 of DeepSeek-R1. While the Output heads (blue dots) are consistently high-rank, many Value heads (orange dots) have a very low effective rank, dragging the fused rank (green line) down with them.\n"
      ],
      "metadata": {
        "id": "0X_rFze2UGdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://lh3.googleusercontent.com/d/1A4whsqz0PRgxR0fa3n73XTpSR0XfwUo9' alt='Plot of the effective ranks of the Value, Output, and fused VO matrix in DeepSeek-R1 showing the combined effective rank being dragged down by the value matrix' width='900' />\n"
      ],
      "metadata": {
        "id": "8J_mqHSZAaQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 2: Only Low When Fused**\n",
        "\n",
        "More interestingly, it's possible for both matrices in a pair to have high effective rank individually, yet produce a fused matrix with a much lower rank.\n",
        "\n",
        "Layer 2 of DeepSeek-R1 provides a prominent example. Both the Value and Output heads appear to be high-rank, but their combined effective rank is dramatically lower.\n",
        "\n"
      ],
      "metadata": {
        "id": "LgvkY_qlAMAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://lh3.googleusercontent.com/d/1iBfzQ1qLt6p5jhHMGaBtD3vob3RAxTZh' alt='Plot of the effective ranks of the Value, Output, and fused VO matrix in DeepSeek-R1 layer 2 showing the combined effective rank being much lower than either of the v or o separately' width='900' />\n"
      ],
      "metadata": {
        "id": "EI7jNR0CANmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has at least two possible causes, and deserves more investigation.\n",
        "\n",
        "The example query-key head pair that we explored (Layer 13 Head 50 of Kimi-K2) exhibited this, and it could predominantly (but not entirely) be attributed to the singular values of the key head being more uniformly low--it appears to be suppressing much of the pair's behavior.\n",
        "\n",
        "Cumulative energy only looks at the magnitude of the singular values relative to one another, so it doesn't capture this when looking at the key head independently.\n",
        "\n",
        "The remainder of the rank drop comes from the interaction between the $W^{QB}(W^{KB})^\\top$ matrices, which define the heads subspaces, and can be \"misaligned\". We'll explore this more in part 2.\n"
      ],
      "metadata": {
        "id": "3RVsVImrEMsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**The Common Case: High-Rank Layers**\n",
        "\n",
        "While the above cases present opportunities, it is crucial to note that they are the exception. In most layers, both the individual and fused matrices are full or near-full rank. Layer 28, shown below, is representative of this common scenario, where FaRR would offer no parameter savings.\n",
        "\n",
        "<img src='https://lh3.googleusercontent.com/d/1FAwlOuRATdVS3xJ1c44FK3_S2nnBpGo8' alt='Plot of the effective ranks of the Value, Output, and fused VO matrix in layer 28 of DeepSeek-R1 showing high rank for all three' width='700' />\n"
      ],
      "metadata": {
        "id": "jqtCH6ySEMDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. A Model-Wide View\n"
      ],
      "metadata": {
        "id": "9LM5GrgQAVHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having examined individual layer patterns, we now zoom out to view the entire DeepSeek-R1 model. The grid plots below show the effective rank (at 99.9% energy) for all Value-Output and Query-Key head pairs. Each plot sorts the heads within a layer by their fused effective rank (the green line) to reveal the distribution of compression opportunities. The horizontal axis represents the 128 heads in the layer, and the vertical axis is the effective rank, capped at 128.\n"
      ],
      "metadata": {
        "id": "28Jmeq0sETte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value-Output (VO) Heads**\n",
        "\n",
        "<img src='https://mccormickml.com/assets/svd_fusion/vo_heads_all_layers_ds-r1.png' alt='Plot showing, for all 61 layers of DeepSeek R1, the effective rank of the value and output heads and their fused form. Sorted by fused effective rank. Early layers show low rank, and some at the end, but the bulk of the middle appears very high.' width='900' />\n"
      ],
      "metadata": {
        "id": "9J9EGNGUFojL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Several key patterns emerge from this model-wide view:\n",
        "\n",
        "  * **Most layers are high-rank.** The central block of the model, roughly layers 20–50, consists almost entirely of full-rank heads where FaRR would be ineffective.\n",
        "  * **Opportunities are concentrated in early and late layers.** Significant low-rank structure is present in the first \\~15 layers and again near the final layer.\n",
        "  * **A curious split emerges in early layers.** Layers 4–8 exhibit a consistent split where roughly half the heads show the \"bottleneck effect\" (low-rank Value head) and the other half show the \"redundant subspace\" pattern (both V/O high-rank, but low fused rank). This bimodal distribution of behavior within a single layer is remarkable.\n"
      ],
      "metadata": {
        "id": "WIYeki7WF23j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query-Key (QK) Heads**\n",
        "\n"
      ],
      "metadata": {
        "id": "bSzHEq3yGGJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Grid plot showing all 61 layers of DeepSeek-R1, plotting the effective rank of all query and key heads and their fused form. Might be more low rank heads overall than for value-output, but still plenty of layers with high rank heads.](https://mccormickml.com/assets/svd_fusion/qk_heads_all_layers_ds-r1.png)"
      ],
      "metadata": {
        "id": "Nq4A5QARGILe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Query-Key heads show even more pronounced low-rank structure in the early layers. Layer 8, for instance, appears to have a uniformly low effective rank across all heads, making it an ideal candidate for FaRR. However, other layers like 5 and 7 are frustratingly close, with a small fraction of high-rank heads preventing a simple, uniform truncation across the entire layer.\n"
      ],
      "metadata": {
        "id": "0EA9kkpWGHi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**The Practical Challenge: Mismatched Head Sizes and Parallelism**\n",
        "\n",
        "These plots reveal the primary obstacle to applying FaRR for inference speedups--heterogenous rank.\n",
        "\n",
        "FlashAttention requires that the query, key, and value heads in a layer all be of identical size.\n",
        "\n",
        "To adhere to our chosen energy threshold, then, we cannot truncate any lower than the _single highest rank head_ in a layer, whether that's on the $W^{QK}$ side or the $W^{VO}$ side.\n",
        "\n",
        "To apply FaRR cleanly, we would like to see _all_ heads within a layer have low rank."
      ],
      "metadata": {
        "id": "PKFu8PohGsmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Opportunities**\n",
        "\n",
        "Opportunities still exist, but require compromise.\n",
        "\n",
        "Note that:\n",
        "\n",
        "1. From a compute perspective, any parameter reduction achieved via FaRR is _free_, because we are not introducing any new calculations. There is no \"break-even\" point we need to pass, and no added overhead we need to amortize.\n",
        "2. 99.9% is a fairly conservative threshold, and fine-tuning can help recover lost performance.\n",
        "3. We can also deviate from point 1--In the same way that a tipping point exists for breaking a large matrix into two smaller ones, there is a break-even point for running two attention passes at two sizes.\n"
      ],
      "metadata": {
        "id": "1B8TKhLEGrMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## S3. Conclusion"
      ],
      "metadata": {
        "id": "455LuSNxUJbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(TODO)"
      ],
      "metadata": {
        "id": "PRGyiAwhULWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n"
      ],
      "metadata": {
        "id": "ex7rExyt5eop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.1. Mapping to Standard SVD\n"
      ],
      "metadata": {
        "id": "vIhHwQ1I5gS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Standard compact SVD is defined as:\n",
        "\n",
        "$$\n",
        "\\text{SVD}(W) = U\\,\\Sigma\\,V^\\top\n",
        "$$\n",
        "\n",
        "Where,\n",
        "* $U \\in \\mathbb{R}^{d_\\text{output}\\times r} \\quad$ the left singular vectors, spanning the **output** space of $W$\n",
        "* $\\Sigma \\in\\mathbb{R}^{r\\times r} \\quad$ the singular values of $W$.\n",
        "* $V^\\top \\in\\mathbb{R}^{r\\times d_\\text{input}} \\quad$ the right singular vectors, spanning the **input** space of $W$.\n"
      ],
      "metadata": {
        "id": "HY5hAbHiRvMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping to $SVD_\\text{QKV}$**\n",
        "\n",
        "Our A/B notation corresponds to:\n",
        "\n",
        "$$\n",
        "\\boxed{\\,W^{QA} = U,\\quad \\sigma^Q = \\mathrm{diag}(\\Sigma),\\quad W^{QB} = V^\\top\\,}\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "and we write $W = W^{QA}\\,\\sigma^Q\\,W^{QB}$ instead of $U\\Sigma V^\\top$.\n",
        "\n",
        "\n",
        "All row/column scaling by $\\sigma$ is implicit based on its placement relative to the matrix.\n"
      ],
      "metadata": {
        "id": "T7A6TRIu7gYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Deep Learning Libraries**"
      ],
      "metadata": {
        "id": "7FUMfS1g7sim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch returns `U, S, Vh` with shapes:\n",
        "\n",
        "* `U` = `[d_model, r]` (orthonormal columns) → $W^{QA}$\n",
        "* `S` = `[r]` (descending singular values) → $\\sigma$\n",
        "* `Vh` = `[r, d_head]` (orthonormal rows) → $W^{QB}$\n",
        "\n",
        "So:\n",
        "\n",
        "```python\n",
        "W_qa, sigma_q, W_qb = torch.linalg.svd(W_q, full_matrices=False)\n",
        "# W_q  =  W_qa @ (W_qb * sigma.unsqueeze(1))  =  (W_qa * sigma.unsqueeze(0)) @ W_qb\n",
        "```\n"
      ],
      "metadata": {
        "id": "k89IozDA7ZjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - We could further simplify by defining a function:"
      ],
      "metadata": {
        "id": "d3ramlEm8HED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "@torch.no_grad()\n",
        "def svd_qkv(W: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Decompose a query, key, or value attention head into A/B + sigma, using our notation.\n",
        "    Returns: W_a, sigma_w, W_b  (columns-ortho, vector, rows-ortho)\n",
        "    Example:\n",
        "    W_qa, sigma_q, W_qb = svd_qkv(W_q)\n",
        "    \"\"\"\n",
        "    W_a, sigma_w, W_b = torch.linalg.svd(W, full_matrices=False)\n",
        "    # (Optional) sanity checks you might enable during development:\n",
        "    # assert torch.allclose(W_qa.T @ W_qa, torch.eye(W_qa.shape[1]), atol=1e-5)\n",
        "    # assert torch.allclose(W_qb @ W_qb.T, torch.eye(W_qb.shape[0]), atol=1e-5)\n",
        "    return W_a, sigma_w, W_b\n",
        "```\n"
      ],
      "metadata": {
        "id": "0Fl-VCI_8EVP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI63Gdo_B9Cr"
      },
      "source": [
        "## A.2. Fusion Derivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RARBELO3o_1W"
      },
      "source": [
        "Let’s start from the standard single-head attention equations for a query token embedding $x_q \\in \\mathbb{R}^{d_\\text{model}}$ and a sequence of token embeddings $X \\in \\mathbb{R}^{T \\times d_\\text{model}}$:\n",
        "\n",
        "**Queries, Keys, Values:**\n",
        "\n",
        "$$\n",
        "q = x_q W^Q_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "K = X W^K_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "V = X W^V_i\n",
        "$$\n",
        "\n",
        "**Attention scores and weighted sum:**\n",
        "\n",
        "$$\n",
        "\\alpha = \\mathrm{softmax}\\!\\left(\\frac{q K^\\top}{\\sqrt{d_k}}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "z = \\alpha V\n",
        "$$\n",
        "\n",
        "where $z \\in \\mathbb{R}^{1 \\times d_v}$ is in the head’s value space.\n",
        "\n",
        "**Output projection:**\n",
        "\n",
        "$$\n",
        "o = z W^O_i\n",
        "$$\n",
        "\n",
        "with $o \\in \\mathbb{R}^{1 \\times d_\\text{model}}$ in the full embedding space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEkt7oKBo_1W"
      },
      "source": [
        "**Moving $W^O_i$ before the attention weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLJJw9GCo_1W"
      },
      "source": [
        "Because matrix multiplication is associative, we can fuse $W^V_i$ and $W^O_i$ before applying the attention weights:\n",
        "\n",
        "$$\n",
        "o = \\alpha \\, (V W^O_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\phantom{o} = \\alpha \\, (X W^V_i W^O_i)\n",
        "$$\n",
        "\n",
        "This gives us the **Value–Output fused matrix**:\n",
        "\n",
        "$$\n",
        "\\boxed{W^{VO}_i = W^V_i W^O_i}\n",
        "$$\n",
        "\n",
        "which maps directly from the model space to the model space through a $d_v$-dimensional bottleneck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRvKtEKbo_1X"
      },
      "source": [
        "**Query–Key fusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQIAvKi7o_1X"
      },
      "source": [
        "Similarly, in the score computation:\n",
        "\n",
        "$$\n",
        "\\alpha = \\mathrm{softmax}\\!\\left(\\frac{(x_q W^Q_i) (X W^K_i)^\\top}{\\sqrt{d_k}}\\right)\n",
        "$$\n",
        "\n",
        "we can regroup:\n",
        "\n",
        "$$\n",
        "(x_q W^Q_i)(X W^K_i)^\\top\n",
        "= x_q \\, \\big[ W^Q_i (W^K_i)^\\top \\big] \\, X^\\top\n",
        "$$\n",
        "\n",
        "so:\n",
        "\n",
        "$$\n",
        "\\boxed{W^{QK}_i = W^Q_i (W^K_i)^\\top}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYd0Hiwo_1X"
      },
      "source": [
        "## A.3. RoPE Head Fusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04cdeccf"
      },
      "source": [
        "If we want to analyze the shared subspace between $W^Q$ and $W^K$, we might try to fuse them into a single matrix:\n",
        "\n",
        "$$\n",
        "W^{QK}_i = W^Q_i \\cdot W^{K\\top}_i\n",
        "$$\n",
        "\n",
        "That works fine **if** $Q$ and $K$ are computed directly from the same linear projection of $X$.\n",
        "\n",
        "But with RoPE, the process is:\n",
        "\n",
        "$$\n",
        "Q_\\text{rot} = \\text{RoPE}(X W^Q), \\quad K_\\text{rot} = \\text{RoPE}(X W^K)\n",
        "$$\n",
        "\n",
        "RoPE is a **token-position–dependent rotation** in each query/key vector’s 2D subspaces.\n",
        "It’s **not linear**, so you can’t simply push it through or ignore it:\n",
        "\n",
        "* The rotation is different for each token position $p$.\n",
        "* It changes the basis directions themselves, meaning the fused $W^{QK}$ would need to “know” about position $p$ to be correct.\n",
        "\n",
        "Can we work around this?\n",
        "\n",
        "If a model uses **partial RoPE** (e.g., only the first $d_\\text{rope}$ dims per head), you can fuse and analyze the remaining “NoPE” dimensions exactly like $W^{VO}$. These dimensions see no rotation, so QK fusion is valid there.\n",
        "\n",
        "This is the approach we took for this article.\n",
        "\n",
        "Also, a technique which can still allow for fused rank analysis (but not FaRR) is to fuse with a fixed-offset.\n",
        "\n",
        "If you choose a fixed position offset $\\Delta p$, you can apply RoPE with that offset to both $W^Q$ and $W^K$ **before** fusing:\n",
        "\n",
        "   $$\n",
        "   W^{QK}_\\text{offset}(\\Delta p) = \\text{RoPE}_{p=0}(W^Q) \\cdot \\text{RoPE}_{p=\\Delta p}(W^K)^\\top\n",
        "   $$\n",
        "\n",
        "The result is offset-specific rather than global, so we can't use it for parameter reduction on RoPE heads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWWzdMviSVsV"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    }
  ]
}