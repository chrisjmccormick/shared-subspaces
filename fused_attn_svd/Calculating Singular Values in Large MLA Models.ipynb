{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjmccormick/shared-subspaces/blob/main/fused_attn_svd/Calculating%20Singular%20Values%20in%20Large%20MLA%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBSNM_VBFGyH"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYKiegh3GLok"
      },
      "source": [
        "# S1. Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3alyuz3pK1xe"
      },
      "source": [
        "This Notebook calculates singular values for all of the attention heads in models using the DeepSeek-V3 architecture (DeepSeek-V3, DeepSeek-R1, Kimi-K2, ...)\n",
        "\n",
        "The resulting data structure is designed to be used by a separate plotting notebook, [here]() TODO.\n",
        "\n",
        "Some of the plots are used in my blog post, [here](https://mccormickml.com/2025/07/28/output-latent-spaces-in-multihead-attention/).\n",
        "\n",
        "\n",
        "I give some context / introduction to SVD analysis in my blog post (again, [here](https://mccormickml.com/2025/07/28/output-latent-spaces-in-multihead-attention/)) and am working on a more in-depth tutorial.\n",
        "\n",
        "All of the analysis is based on the singular values of these weight matrices, so I start by computing and saving all of those (in this notebook).\n",
        "\n",
        "I've pre-computed singular values for several models, so if you want to play with them yourself you can skip the whole process of retrieving the V3 weights and running SVD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kdkq3FLzFL"
      },
      "source": [
        "## 1.1. Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pj66ZouJIaF"
      },
      "source": [
        "### Lower Effective Rank of Fused Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z57Dp-LwaJA6"
      },
      "source": [
        "We're looking at the fused forms of the attention matrices because the fused form often has a lower effective rank than the individual heads.\n",
        "\n",
        "How does that happen? This deserves its own blog post, but I'll try to give a quick overview.\n",
        "\n",
        "Let's use the dimensions of GPT-2 as an example. It has an embedding size (a.k.a. the size of its \"residual stream\") of 768, and a head size of 64.\n",
        "\n",
        "The way I would typically think about the respective roles of a given Value head and Output head pair is that the 64 vectors in the Value head dictate the directions in the residual stream it can read from, and for the Output head they determine the directions it can write to.\n",
        "\n",
        "In order to make sense of this lower rank phenomemom, it helps to think instead of the Value head writing into a 64 dimensional space and the Output head reading from a 64 dimensional space.\n",
        "\n",
        "What's a little odd about this is that they both independently define that 64-dim space.\n",
        "\n",
        "In the residual stream, I think it makes intuitive sense that the heads might want to read and write to different parts of it.\n",
        "\n",
        "But in this 64-dim inner space, if the Value head and Output head don't \"agree\" on the definition of the space, then we get lower effective rank heads and therefore poorer use of the available parameters.\n",
        "\n",
        "There are two types of scenarios where this happens:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORIFQ6xUeSyV"
      },
      "source": [
        "**Dragged Down by the Lower of the Two**\n",
        "\n",
        "The first is where, say, the Output head has full rank, but the Value head has relatively low rank, and their fused form has the same rank as the Value head.\n",
        "\n",
        "You can see this in many of the early layers. One strong example is layer 8 of DeepSeek-R1, below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhWqTqVTdl38"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1A4whsqz0PRgxR0fa3n73XTpSR0XfwUo9' alt='Plot of the effective ranks of the Value, Output, and fused VO matrix in DeepSeek-R1 showing the combined effective rank being dragged down by the value matrix' width='900' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geWLzxJmdiOQ"
      },
      "source": [
        "\n",
        "This means that the Output head is capable of reading from a larger space than the Value head can actually write to, but that they are at least in agreement on the space in the sense that where the Value head can write is (largely) encompassed by what the Output head can read.\n",
        "\n",
        "In this case, the Output head has more parameters than it needs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tAKoFQ5eLrY"
      },
      "source": [
        "**High-Rank but Misaligned**\n",
        "\n",
        "The second situation I find more interesting. Let's say the Value and Output heads are both showing high effective rank, but their fused form has much lower rank. What happened?\n",
        "\n",
        "The first few layers seem to have the most examples of this. Below is layer 2 of DeepSeek-R1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwdEg9W6epTU"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1iBfzQ1qLt6p5jhHMGaBtD3vob3RAxTZh' alt='Plot of the effective ranks of the Value, Output, and fused VO matrix in DeepSeek-R1 layer 2 showing the combined effective rank being much lower than either of the v or o separately' width='900' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbKs0CReodh"
      },
      "source": [
        "This indicates that the heads aren't aligned on their definition of the 64-dim space that they communicate through.\n",
        "\n",
        "The Value head is writing strongly to directions that the Output head isn't strongly reading from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stjkzDDreuUF"
      },
      "source": [
        "**Similar Outcomes**\n",
        "\n",
        "Both scenarios have the same effect--there's a smaller amount of overlap between the Value and Output head. So perhaps they're no different, and the second scenario is just a different way of arriving at the first.\n",
        "\n",
        "Just speculation, but I wonder if the second scenario occurs when the model learns a behavior in the head earlier on in training, resulting in a pair of high-rank heads, but then decides to suppress it later, and rotates the spaces out of alignment?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrxlQAr7ivWP"
      },
      "source": [
        "**High Rank Heads**\n",
        "\n",
        "I think it's also worth noting that in many cases (most, really!) the Value and Output heads in a pair are both high rank, and so is their fused form.\n",
        "\n",
        "The plots usually look more like this one--it's the early layers where we see most of the low effective rank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dogYKQEKd8Vf"
      },
      "source": [
        "<img src='https://lh3.googleusercontent.com/d/1FAwlOuRATdVS3xJ1c44FK3_S2nnBpGo8' alt='Plot of the effective ranks of the Value, Output, and fused VO matrix in layer 28 of DeepSeek-R1 showing high rank for all three' width='700' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYWVYTzpd1jS"
      },
      "source": [
        "When the effective ranks are both high and so is the fused form, I think this implies that they've both landed on a well-aligned 64-dimensional space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHkvro0gT6K9"
      },
      "source": [
        "### Miscellaneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATNSKGaKHyWu"
      },
      "source": [
        "**Folding RMSNorm Weight Vectors**\n",
        "\n",
        "The RMSNorms are a non-linear operation, but they do contain a weight vector on the end which can be folded into the matrix following the norm.\n",
        "\n",
        "There's a flag for whether to do this or not.\n",
        "\n",
        "For my analysis, I did fold them in, since there may be additional structure from the combination which could further reduce the effective rank of the projection matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C8iEFC3JF9v"
      },
      "source": [
        "**Spectral Normalization**\n",
        "\n",
        "This is something that needs elaborating on. Prior to stacking the matrices, we're dividing each one by it's largest singular value.\n",
        "\n",
        "This has the effect of normalizing the amplitude of the heads so that any \"loud\" ones don't dominate the analysis.\n",
        "\n",
        "It makes the results more about overlap in the directions, which is probably a good thing for identifying shared subspaces, which was the original point of this analysis.\n",
        "\n",
        "But it is also likely raises the effective ranks, so probably deserves at least calling out somewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOXQY7i9JNOG"
      },
      "source": [
        "### Fusing RoPE Heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVD-zZsMe2O7"
      },
      "source": [
        "Heads which receive position information can't be fused in quite the same way.\n",
        "\n",
        "Their interaction is not as simple as $x W^Q_i {W^K_i}^\\top x^\\top$.\n",
        "\n",
        "The correct equation needs to include the RoPE embeddings in between, which are dependent on the position offset $\\Delta$ between the two tokens you're comparining.\n",
        "\n",
        "So now the fused matrix has this extra parameter to incorporate:\n",
        "\n",
        "$$\\,W^{QK}_i(\\Delta)=W^Q_i\\, R(\\Delta)\\,{W^K_i}^{\\!\\top}$$\n",
        "\n",
        "and we can't trivially fuse it like we have been.\n",
        "\n",
        "However, if we choose a particular offset, e.g., 128, then we _can_ fuse them and look at the matrix at least within that context.\n",
        "\n",
        "What's great about MLA, though, is that it doesn't apply RoPE to most of its query and key head dimensions! That's why we're able to calculate their fused form in this Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq7RSfs8L18G"
      },
      "source": [
        "## 1.2. Practical Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ZGtu5KLKDX"
      },
      "source": [
        "**File Management**\n",
        "\n",
        "DeepSeek-V3/R1 and Kimi-K2 are enormous models, something like 650B parameters for DS-V3 and 1 Trillion for Kimi-K2! Even though the weights have been quantized to 8-bits, they're still too big to fit on the 250GB hard drive of a Colab instance!\n",
        "\n",
        "For smaller models, I would simply use the `from_pretrained` function in HuggingFace `transformers` to retrieve and load the model weights.\n",
        "\n",
        "Instead, we need to carefully manage the disk space and download and delete files as needed. The functions for this are in the \"Retrieval Functions\" section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lxQsV02MGnw"
      },
      "source": [
        "**Copmute Requirements**\n",
        "\n",
        "Since we're looking at the matrices one at a time, the free T4 instances have enough GPU memory to perform the task. They seem to have a slower connection to the huggingface models repo, though, which is an important factor here. If you have Colab Pro, the L4 may be a better choice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X9oJL9bHyWu"
      },
      "source": [
        "**Reference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76lC419LHyWu"
      },
      "source": [
        "I went off of my commented version of MLA (TODO - share) to figure out / verify I was accessing the parameter shapes correctly.\n",
        "\n",
        "Note that there are two implementations--one from DeepSeek and one from HuggingFace. The parameter names come from the HuggingFace version, but my commented version is the DeepSeek one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLIfjQjTSUCU"
      },
      "source": [
        "**Data Structure**\n",
        "\n",
        "Specific weight matrices are indexed using a letter code:\n",
        "\n",
        "```python\n",
        "# Symbols for all of the weight matrices\n",
        "W_names = [\n",
        "    'Q', 'K', 'V', 'O', # Attention matrices\n",
        "    'QK', 'KQ', 'VO',   # Fused matrices\n",
        "    'KVA', 'QA',        # Latent projections\n",
        "    'KVA_pe', 'Q_pe'    # Position information (RoPE) heads.\n",
        "]\n",
        "```\n",
        "\n",
        "Then the singular values are stored in dictionaries:\n",
        "\n",
        "```python\n",
        "# Subspace matrices are one per-layer\n",
        "S_subspaces = {}     # S_subspaces[layer_i][W_name] --> List of sigmas\n",
        "\n",
        "# Calculated for each head independently (i.e., if DS-V3 has 128 heads\n",
        "# in a layer, so there are 128 sets of singular values)\n",
        "S_heads = {}         #     S_heads[layer_i][W_name][head_i] --> List of sigmas\n",
        "\n",
        "# For a particular parameter, stack all of the heads to find a common\n",
        "# subspace. One stack per layer.\n",
        "S_stacked_heads = {} # S_stacked_heads[layer_i][W_name] --> List of sigmas\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggs6jvfBDKal"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaNhMguZDBPc"
      },
      "source": [
        "# S2. Calculate Singular Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUdgS86OIVaS"
      },
      "source": [
        "## 2.1. Choose Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjQzTgd0IdsT"
      },
      "source": [
        "This section handles defining some global variables depending on what model you want to look at. Set the `model_name` variable below and it will initialize the right values.\n",
        "\n",
        "Note that I only have the pre-computed values uploaded for one model currently, DeepSeek-R1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvi7zm57bgfp",
        "outputId": "54a4f00c-ad59-4fa6-8329-9e4f995a0672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: DS-V3-base | Layers: 62 | Heads: 128\n",
            "Singular values file: ds-v3-base_singular_values.pkl\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "#   Choose a Model To Look At\n",
        "# ==============================\n",
        "\n",
        "# Choose the model you want to look at:\n",
        "#   Options: \"DS-V3\", \"DS-V3-base\", \"DS-R1\", \"K2\", \"K2-base\"\n",
        "model_name = \"DS-V3-base\"\n",
        "\n",
        "# DeepSeek-V3, Instruction-tuned\n",
        "if model_name == \"DS-V3\":\n",
        "    singular_vals_file = \"ds-v3_singular_values.pkl\"\n",
        "    num_layers = 62   # includes MTP head\n",
        "    num_heads = 128\n",
        "\n",
        "    vals_available = False\n",
        "\n",
        "    repo_id = \"deepseek-ai/DeepSeek-V3\"\n",
        "    safetensors_index_filename = \"inference/model.safetensors.index.json\"\n",
        "\n",
        "# DeepSeek-V3 Base model\n",
        "elif model_name == \"DS-V3-base\":\n",
        "    singular_vals_file = \"ds-v3-base_singular_values.pkl\"\n",
        "    num_layers = 62\n",
        "    num_heads = 128\n",
        "\n",
        "    vals_available = False\n",
        "\n",
        "    repo_id = \"deepseek-ai/DeepSeek-V3-Base\"\n",
        "    safetensors_index_filename = \"model.safetensors.index.json\"\n",
        "\n",
        "# DeepSeek-R1 Reasoning model\n",
        "elif model_name == \"DS-R1\":\n",
        "    singular_vals_file = \"ds-r1_singular_values.pkl\"\n",
        "    num_layers = 62\n",
        "    num_heads = 128\n",
        "\n",
        "    vals_available = True\n",
        "\n",
        "    repo_id = \"deepseek-ai/DeepSeek-R1\"\n",
        "    safetensors_index_filename = \"model.safetensors.index.json\"\n",
        "\n",
        "# Kimi-K2 Instruction-tuned\n",
        "elif model_name == \"K2\":\n",
        "    singular_vals_file = \"k2_singular_values.pkl\"\n",
        "    num_layers = 61\n",
        "    num_heads = 64\n",
        "\n",
        "    vals_available = False\n",
        "\n",
        "    repo_id = \"moonshotai/Kimi-K2-Instruct\"\n",
        "    safetensors_index_filename = \"model.safetensors.index.json\"\n",
        "\n",
        "# Kimi-K2 Base model\n",
        "elif model_name == \"K2-base\":\n",
        "    singular_vals_file = \"k2-base_singular_values.pkl\"\n",
        "\n",
        "    num_layers = 61\n",
        "    num_heads = 64\n",
        "\n",
        "    vals_available = False\n",
        "\n",
        "    repo_id = \"moonshotai/Kimi-K2-Base\"\n",
        "    safetensors_index_filename = \"model.safetensors.index.json\"\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "print(f\"Model: {model_name} | Layers: {num_layers} | Heads: {num_heads}\")\n",
        "print(f\"Singular values file: {singular_vals_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0UcFgzMOpF5"
      },
      "source": [
        "Retrieve the index file from the huggingface datasets repository for the model. This maps the parameter names to the name of the safetensor file that contains it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e61f4a720b81411597f1066c68ec9d61",
            "23ddec8ce2ed473cabf4893995b7fab2",
            "fb8d73df90304542b2cb4e850ba58f0f",
            "d36f208197054365bd6c4a3114905aad",
            "76348540c65f4262a48a131a1fc760e2",
            "cdbf7d7e3e7141ff94bed59821891116",
            "e7a267d9cdbc49dcaa8aadd3982665eb",
            "f1010654c5944e938b59f70ea330fdc5",
            "a8930b207d9443248cf28d77be46ae90",
            "82b0b32ad31b49538c87028b7f93c00e",
            "a33586225d874ac597709d729156b4ee"
          ]
        },
        "id": "jWfqdXdUOCl9",
        "outputId": "19ab6ecc-1c52-4ea6-dad3-23833964e21a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e61f4a720b81411597f1066c68ec9d61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the safetensors index json for the selected model.\n",
        "json_path = hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=safetensors_index_filename,\n",
        "    local_dir=\".\",\n",
        "    repo_type=\"model\",   # these repos are model repos\n",
        ")\n",
        "\n",
        "# Load JSON as \"file_mappings\"\n",
        "with open(json_path) as f:\n",
        "    file_mappings = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9rrp0_2O0j-"
      },
      "source": [
        "Calculating the singular values is heavy work, I'd definitely recommend a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbqnpzABOoVH"
      },
      "outputs": [],
      "source": [
        "# Device for SVD computation\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGJo9yayEkgF"
      },
      "source": [
        "## 2.2. Retrieval Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOfKnAE6kiXX"
      },
      "source": [
        "**Dequant Function**\n",
        "\n",
        "The DS-V3 architecture stores most of its weight using 8-bit quantization, so we need to dequantize the weights first.\n",
        "\n",
        "I had GPT write the below function for me based on code from the HuggingFace GitHub in the files:\n",
        "\n",
        "* [fp8_cast_bf16.py](https://huggingface.co/deepseek-ai/DeepSeek-V3/raw/main/inference/fp8_cast_bf16.py)\n",
        "* [kernel.py](https://huggingface.co/deepseek-ai/DeepSeek-V3/raw/main/inference/kernel.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nybkqca4kh6v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def dequant_fp8_blockwise(x: torch.Tensor, scale_inv: torch.Tensor, block_size: int = 128) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Fallback for dequantizing FP8 weights without using Triton.\n",
        "    Assumes x is [M, N], scale_inv is [M // B, N // B]\n",
        "    \"\"\"\n",
        "    M, N = x.shape\n",
        "    M_blocks = M // block_size\n",
        "    N_blocks = N // block_size\n",
        "    y = torch.empty(M, N, dtype=torch.float32)\n",
        "\n",
        "    for i in range(M_blocks):\n",
        "        for j in range(N_blocks):\n",
        "            row_start = i * block_size\n",
        "            row_end = row_start + block_size\n",
        "            col_start = j * block_size\n",
        "            col_end = col_start + block_size\n",
        "\n",
        "            scale = scale_inv[i, j]\n",
        "            block = x[row_start:row_end, col_start:col_end].to(torch.float32)\n",
        "            y[row_start:row_end, col_start:col_end] = block * scale\n",
        "\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soswj_vqPT5b"
      },
      "source": [
        "**Get Disk Usage**\n",
        "\n",
        "We need to monitor our disk usage and delete files as needed--this function retrieves the current disk usage for the Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpEPpxPFXb_i",
        "outputId": "ac2a07f7-c388-4888-dcf4-1d3e28897931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   39G  197G  17% /\n"
          ]
        }
      ],
      "source": [
        "# To get the disk usage in human-readable format\n",
        "!df -h /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv3oGPLEX2mm"
      },
      "outputs": [],
      "source": [
        "# Run \"!df -h /\" and parse the output into CSV\n",
        "import csv\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "def get_disk_usage():\n",
        "    \"\"\"\n",
        "    Returns the current disk usage percentage as an integer.\n",
        "    \"\"\"\n",
        "\n",
        "    result = subprocess.run([\"df\", \"-h\", \"/\"], capture_output=True, text=True)\n",
        "\n",
        "    # Output looks like:\n",
        "    #    Filesystem      Size  Used Avail Use% Mounted on\n",
        "    #    overlay         226G   47G  180G  21% /\n",
        "    #\n",
        "\n",
        "    lines = result.stdout.strip().split(\"\\n\")\n",
        "    headers = lines[0].split()\n",
        "\n",
        "    # Remove the last header\n",
        "    headers = headers[:-1]\n",
        "\n",
        "    data = [line.split() for line in lines[1:]]\n",
        "\n",
        "    # Load into a dataframe\n",
        "    df = pd.DataFrame(data, columns=headers)\n",
        "    #display(df)\n",
        "\n",
        "    value = df.loc[0, \"Use%\"]\n",
        "\n",
        "    # Parse the percentage (e.g., \"21%\") into an integer\n",
        "    usage = int(value[:-1])\n",
        "    #print(usage)\n",
        "\n",
        "    return usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlspzkHgUt4y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_file_number(filename):\n",
        "    \"\"\"Extract integer file number from a safetensors filename like\n",
        "    model-00040-of-000163.safetensors\"\"\"\n",
        "\n",
        "    match = re.search(r'model-(\\d+)-of-\\d+\\.safetensors', filename)\n",
        "\n",
        "    assert match is not None, \"Invalid filename format\"\n",
        "\n",
        "    return int(match.group(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFok5lbAUEDn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "cache_dir = \"./fp8_cache/\"\n",
        "\n",
        "def free_up_space(current_filename):\n",
        "    \"\"\"\n",
        "    Frees disk space by identifying which file we're least likely to need next\n",
        "    (based on the assumption that it's whatever file number if furthest away)\n",
        "    and deleting it.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the file number from the filename.\n",
        "    current_number = extract_file_number(current_filename)\n",
        "\n",
        "    # List all cached safetensors files and find the one furthest from current\n",
        "    candidate_files = [\n",
        "        f for f in os.listdir(cache_dir) if f.startswith(\"model\") and f.endswith(\".safetensors\")\n",
        "    ]\n",
        "\n",
        "    furthest_file = None\n",
        "    max_distance = -1\n",
        "\n",
        "    # For each file,\n",
        "    for f in candidate_files:\n",
        "\n",
        "        # Get the filenumber\n",
        "        file_number = extract_file_number(f)\n",
        "\n",
        "        # Check if it's the furthest away.\n",
        "        distance = abs(current_number - file_number)\n",
        "        if distance > max_distance:\n",
        "            max_distance = distance\n",
        "            furthest_file = f\n",
        "\n",
        "\n",
        "    print(f\"  ↳ Deleting {furthest_file}\")\n",
        "\n",
        "    os.remove(os.path.join(cache_dir, furthest_file))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZushH8KcERK"
      },
      "source": [
        "**`retrieve_and_dequant_weight`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ejVYCGPlUS"
      },
      "source": [
        "\n",
        "This function implements the overall process for retrieving the specified parameter from DeepSeek-V3.\n",
        "\n",
        "It manages disk usage by deleting files as needed to free up space for new ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCIL77riOc4V"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import safe_open\n",
        "\n",
        "\n",
        "def retrieve_and_dequant_weight(param_name, scale_name=None, dequant=True):\n",
        "\n",
        "    #print(f\"Retrieveing {param_name}...\")\n",
        "\n",
        "    # Look up the filename for the requested parameter.\n",
        "    weight_filename = file_mappings[\"weight_map\"][param_name]\n",
        "\n",
        "    # First, check if the disk is getting full, and if so,\n",
        "    # delete an existing safetensors file\n",
        "    usage = get_disk_usage()\n",
        "\n",
        "    # If the use is over 90%...\n",
        "    if usage > 90:\n",
        "        #print(f\"  ↳ Disk usage is {usage}%\")\n",
        "\n",
        "        # Delete the furthest away file from the requested one.\n",
        "        free_up_space(weight_filename)\n",
        "\n",
        "    # Use huggingfaces download function to retrieve it.\n",
        "    weight_path = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=weight_filename,\n",
        "        local_dir=cache_dir\n",
        "    )\n",
        "\n",
        "    # Use `safe_open` from safetensors to get the parameter from the file.\n",
        "    with safe_open(weight_path, framework=\"pt\", device=device) as f:\n",
        "        W = f.get_tensor(param_name)\n",
        "\n",
        "    # Dequantize the weights, if needed.\n",
        "    # (The RMSNorm weight vectors are already 16-bit)\n",
        "    if dequant:\n",
        "\n",
        "        # Load the S_inv parameter which contains the scaling factors to\n",
        "        # dequantize the weights.\n",
        "\n",
        "        # Note that sometimes this parameter crosses a file boundary, and we\n",
        "        # will need to load a separate file.\n",
        "        scale_filename = file_mappings[\"weight_map\"][scale_name]\n",
        "\n",
        "        if scale_filename != weight_filename:\n",
        "            #print(f\"  ↳ Loading scale tensor from separate file.\")\n",
        "            pass\n",
        "\n",
        "        # Downloads the file if needed and gets the path to it.\n",
        "        # The earlier disk usage check should have freed up plenty of space if\n",
        "        # needed.\n",
        "        scale_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=scale_filename,\n",
        "            local_dir=cache_dir)\n",
        "\n",
        "        # Open with safetens\n",
        "        with safe_open(scale_path, framework=\"pt\", device=device) as f:\n",
        "            S_inv = f.get_tensor(scale_name)\n",
        "\n",
        "        # Dequantize\n",
        "        W = dequant_fp8_blockwise(W.cpu(), S_inv.cpu()).to(\"cuda\")\n",
        "\n",
        "        #print(f\"  ↳ Dequantized shape: {W.shape}\")\n",
        "\n",
        "    else:\n",
        "        #print(f\"  ↳ W shape: {W.shape}\")\n",
        "        pass\n",
        "\n",
        "    return W\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwbvPcinYh6x"
      },
      "source": [
        "**Effective Rank at a Given Error**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmdiLdG4mtVk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_rank_for_error_threshold(S_values, error_threshold):\n",
        "    \"\"\"\n",
        "    Calculates the minimum rank required to keep reconstruction error below a threshold.\n",
        "\n",
        "    Args:\n",
        "        S_values (np.array): A 1D array of singular values, sorted descending.\n",
        "        error_threshold (float): The maximum allowed fraction of energy loss\n",
        "                                 (e.g., 0.01 for 1% loss).\n",
        "\n",
        "    Returns:\n",
        "        int: The required rank.\n",
        "    \"\"\"\n",
        "    # Calculate the energy (squared singular values)\n",
        "    energy = S_values**2\n",
        "    total_energy = np.sum(energy)\n",
        "\n",
        "    # The reconstruction error is the fraction of energy lost.\n",
        "    # We calculate the cumulative energy kept, and subtract from 1.\n",
        "    cumulative_energy_kept = np.cumsum(energy)\n",
        "    fraction_of_energy_lost = 1.0 - (cumulative_energy_kept / total_energy)\n",
        "\n",
        "    # Find the first rank where the error is at or below the threshold.\n",
        "    # np.where returns a tuple of arrays; we need the first element of the first array.\n",
        "    valid_ranks = np.where(fraction_of_energy_lost <= error_threshold)[0]\n",
        "\n",
        "    if len(valid_ranks) == 0:\n",
        "        # If no rank satisfies the threshold, it means we need all of them.\n",
        "        return len(S_values)\n",
        "\n",
        "    # Add 1 because rank is 1-based, index is 0-based.\n",
        "    required_rank = valid_ranks[0] + 1\n",
        "    return required_rank\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtsS1c2Efc-a"
      },
      "source": [
        "**Sizes and Shapes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBJh6jm4ffN-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def format_size(num: int) -> str:\n",
        "    \"\"\"Return a human readable string for the given integer.\"\"\"\n",
        "    suffixes = [\" \", \"K\", \"M\", \"B\"]\n",
        "    base = 1024\n",
        "    for suffix in suffixes:\n",
        "        if abs(num) < base:\n",
        "            if num % 1 != 0:\n",
        "                return f\"{num:.2f}{suffix}\"\n",
        "            else:\n",
        "                return f\"{num:.0f}{suffix}\"\n",
        "        num /= base\n",
        "    if num % 1 != 0:\n",
        "        return f\"{num:.2f}T\"\n",
        "    return f\"{num:.0f}T\"\n",
        "\n",
        "def format_shape(shape: torch.Size) -> str:\n",
        "    \"\"\"Return a human readable string for the given shape.\"\"\"\n",
        "\n",
        "\n",
        "    # Working backwards, delete any dimensions that are size 1.\n",
        "    for i in range(len(shape) - 1, -1, -1):\n",
        "        if shape[i] == 1:\n",
        "            del shape[i]\n",
        "\n",
        "    if len(shape) == 1:\n",
        "        shape_str = \"{:>4,} x {:<4}\".format(shape[0], \"-\")\n",
        "    elif len(shape) == 2:\n",
        "        shape_str = \"{:>4,} x {:<4,}\".format(shape[0], shape[1])\n",
        "    elif len(shape) == 3:\n",
        "        shape_str = \"{:>4,} x {:,} x {:<4}\".format(shape[0], shape[1], shape[2])\n",
        "    elif len(shape) == 4:\n",
        "        shape_str = \"{:>4,} x {:,} x {:,} x {:<4}\".format(\n",
        "            shape[0], shape[1], shape[2], shape[3]\n",
        "        )\n",
        "    else:\n",
        "        print(\"Unexpected: \", shape)\n",
        "\n",
        "    return shape_str\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEyY7ZR2PIgs"
      },
      "source": [
        "## 2.3. Resume From Partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUNtel5OQGm3"
      },
      "source": [
        "As a simple save-and-resume mechanic, the current results are pickled at the end of each layer. If something happens, you can reload from the pickle file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAVAD46VPOUT"
      },
      "outputs": [],
      "source": [
        "# Set to True to load from an existing file.\n",
        "resume = True\n",
        "\n",
        "# The filename to store the data in was chosen up in the models section, but\n",
        "# you can also overwrite it here if you want.\n",
        "#singular_vals_file = \"./ds-v3-base_singular_values.pkl\"\n",
        "\n",
        "# If we're resuming from an existing file,\n",
        "if resume:\n",
        "    # Load the dictionaries from the pickle file.\n",
        "    with open(singular_vals_file, \"rb\") as f:\n",
        "        S_subspaces, S_heads, S_stacked_heads = pickle.load(f)\n",
        "\n",
        "    # Set the start layer based on how many layers were completed.\n",
        "    start_layer = len(S_subspaces)\n",
        "\n",
        "# Otherwise, for a fresh run,\n",
        "else:\n",
        "    # Initialize the empty dictionaries.\n",
        "    S_subspaces = {} # S_subspaces[layer_i][W_name] --> List of sigmas\n",
        "    S_heads = {}     # S_heads[layer_i][W_name][head_i] --> List of sigmas\n",
        "    S_stacked_heads = {} # S_stacked_heads[layer_i][W_name] --> List of sigmas\n",
        "\n",
        "    start_layer = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct_Wlh66Rez5"
      },
      "source": [
        "**Google Drive backups**\n",
        "\n",
        "If you're running this on Colab, there's always the potential you'll get disconnected.\n",
        "\n",
        "After processing a layer, there's the option to copy the updated pickle file to your Google Drive so you don't lose it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fppg4-0Rhof",
        "outputId": "0bfa8f12-1035-4de5-d251-be5b2193b42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Set to True to make a backup on Google Drive of the pickle file after\n",
        "# completing each layer.\n",
        "backup_to_gdrive = True\n",
        "\n",
        "if backup_to_gdrive:\n",
        "    # Mount Google Drive.\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Specify the file path for the backup copy.\n",
        "    backup_file = f\"/content/drive/MyDrive/{singular_vals_file}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBuEGxVugjZQ"
      },
      "source": [
        "## 2.4. Analysis Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl90JIABSOzP"
      },
      "source": [
        "Loop through the layers and compute all of the singular values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfVmB8gxHyWv"
      },
      "outputs": [],
      "source": [
        "# Import for reporting elapsed time\n",
        "import time\n",
        "\n",
        "import re\n",
        "\n",
        "# For saving sigma values to disk.\n",
        "import pickle\n",
        "\n",
        "fold_norms = True\n",
        "\n",
        "# Symbols for all of the weight matrices\n",
        "W_names = [\n",
        "    'Q', 'K', 'V', 'O', # Attention matrices\n",
        "    'QK', 'VO',         # Fused matrices\n",
        "    'KVA', 'QA',        # Latent projections\n",
        "    'KVA_pe', 'Q_pe'    # Position information (RoPE) heads.\n",
        "]\n",
        "\n",
        "# For clearer indexing.\n",
        "final_layer = num_layers - 1\n",
        "\n",
        "# Examples:\n",
        "#   To retrieve the singular values for all W^O heads in layer_i:\n",
        "#     S_stacked_heads[layer_i]['O']\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# For each layer in the model...\n",
        "for layer_i in range(start_layer, num_layers):\n",
        "\n",
        "    # Holds the shared subspace weight matrices KV, QA, and KVA_pe for this\n",
        "    # layer. (Note - KVA_pe is really just a single k_pe head)\n",
        "    subspace_ws = {}\n",
        "\n",
        "    # Holds the tensors for the attention heads in this layer.\n",
        "    head_ws = {}\n",
        "\n",
        "    # Create empty dictionaries\n",
        "    S_subspaces[layer_i] = {}\n",
        "    S_heads[layer_i] = {}\n",
        "    S_stacked_heads[layer_i] = {}\n",
        "\n",
        "    print(\"\\n--------\")\n",
        "    print(f\"Layer {layer_i}\")\n",
        "    print(\"--------\\n\")\n",
        "\n",
        "    # Print elapsed time in minutes\n",
        "    print(f\"Elapsed: {(time.time() - t0)/60:.2f} minutes\")\n",
        "\n",
        "    # ======== Attention Input Norm ========\n",
        "    # Retrieve the input layer norm in case we want to fold it into the\n",
        "    # weights.\n",
        "    if fold_norms:\n",
        "        matrix_name = \"input_layernorm\"\n",
        "        param_name = f\"model.layers.{layer_i}.input_layernorm.weight\"\n",
        "\n",
        "        # Retrieve the layernorm on the input to attention.\n",
        "        # It's length 7168, and type bfloat16.\n",
        "        input_norm_weight = retrieve_and_dequant_weight(\n",
        "            param_name,\n",
        "            \"\", # No scale_inv parameter.\n",
        "            dequant=False\n",
        "        )\n",
        "\n",
        "        # Cast to Float\n",
        "        input_norm_weight = input_norm_weight.float()\n",
        "\n",
        "    # ======== Query Subspace Projection =========\n",
        "\n",
        "    param_name = f\"model.layers.{layer_i}.self_attn.q_a_proj.weight\"\n",
        "    scale_name = f\"{param_name}_scale_inv\"\n",
        "\n",
        "    # Shape: 7168, 1536\n",
        "    W_qa = retrieve_and_dequant_weight(\n",
        "        param_name,\n",
        "        scale_name,\n",
        "        dequant=True\n",
        "    )\n",
        "\n",
        "    if fold_norms:\n",
        "        # input_norm_weight is length 7168\n",
        "        # W_QA is 1536, 7168\n",
        "\n",
        "        # Fold the input RMSNorm into the query latent projection, W_qa\n",
        "        W_qa = W_qa * input_norm_weight.unsqueeze(0)\n",
        "\n",
        "    # Store the query latent projection for the current layer.\n",
        "    subspace_ws['QA'] = W_qa\n",
        "\n",
        "    # ======== Queries - RoPE and Non-RoPE =========\n",
        "    matrix_name = \"q_b_proj\"\n",
        "    param_name = f\"model.layers.{layer_i}.self_attn.{matrix_name}.weight\"\n",
        "    scale_name = f\"{param_name}_scale_inv\"\n",
        "\n",
        "    # W.shape: 24576, 1536\n",
        "    W = retrieve_and_dequant_weight(\n",
        "        param_name,\n",
        "        scale_name,\n",
        "        dequant = True\n",
        "    )\n",
        "\n",
        "    # Retrieve the query subspace layernorm and fold it\n",
        "    # into the query projection.\n",
        "    if fold_norms:\n",
        "        if (layer_i == final_layer) or (layer_i == 0):\n",
        "            print(\"    Folding in query subspace layernorm\")\n",
        "\n",
        "        matrix_name = \"q_a_layernorm\"\n",
        "        param_name = f\"model.layers.{layer_i}.self_attn.{matrix_name}.weight\"\n",
        "\n",
        "        # The layernorm weight is type bfloat16 and doesn't need to be dequantized.\n",
        "        # There is no bias term on the RMSNorms.\n",
        "        q_norm_weight = retrieve_and_dequant_weight(\n",
        "            param_name,\n",
        "            \"\", # No scale needed\n",
        "            dequant=False\n",
        "        )\n",
        "\n",
        "        # Cast to float\n",
        "        q_norm_weight = q_norm_weight.float()\n",
        "\n",
        "        if (layer_i == final_layer) or (layer_i == 0):\n",
        "            print(\"    input_norm_weight.shape:\", input_norm_weight.shape)\n",
        "            print(\"    q_norm_weight.shape:\", q_norm_weight.shape)\n",
        "            print(\"    Pre-fold, W_q.shape:\", W.shape)\n",
        "\n",
        "        # Fold into projection matrix\n",
        "        # elementwise (1536,)\n",
        "        # q_norm_weight is length 1536\n",
        "        # W_Q is 24576, 1536\n",
        "\n",
        "        W = W * q_norm_weight.unsqueeze(0)  # shape: [1, 1536]\n",
        "\n",
        "        if (layer_i == final_layer) or (layer_i == 0):\n",
        "            print(\"    Post-fold, W_q.shape:\", W.shape)\n",
        "\n",
        "    # First, reshape into heads.\n",
        "    W = W.view(num_heads, 192, 1536)\n",
        "\n",
        "    # Next, split the heads into the query heads and RoPE query heads\n",
        "    W_q_heads, W_q_pe_heads = torch.split(W, [128, 64], dim=1)\n",
        "    head_ws['Q'] = W_q_heads\n",
        "    head_ws['Q_pe'] = W_q_pe_heads\n",
        "\n",
        "    # ======== KV Subspace Projection =========\n",
        "\n",
        "    param_name = f\"model.layers.{layer_i}.self_attn.kv_a_proj_with_mqa.weight\"\n",
        "    scale_name = f\"{param_name}_scale_inv\"\n",
        "\n",
        "    # Shape: 576, 7168\n",
        "    W_kva_pe = retrieve_and_dequant_weight(\n",
        "        param_name,\n",
        "        scale_name,\n",
        "        dequant=True\n",
        "    )\n",
        "\n",
        "    W_kva, W_pe = torch.split(W_kva_pe, [512, 64], dim=0)\n",
        "\n",
        "    # Subspaces are one per layer\n",
        "    subspace_ws['KVA'] = W_kva\n",
        "    subspace_ws['KVA_pe'] = W_pe\n",
        "\n",
        "    # ======== Keys and Values ========\n",
        "    matrix_name = \"kv_b_proj\"\n",
        "    param_name = f\"model.layers.{layer_i}.self_attn.{matrix_name}.weight\"\n",
        "    scale_name = f\"{param_name}_scale_inv\"\n",
        "\n",
        "    W = retrieve_and_dequant_weight(param_name, scale_name)\n",
        "\n",
        "    # Retrieve the KV subspace layernorm and fold it\n",
        "    # into the key and value projections.\n",
        "    if fold_norms:\n",
        "\n",
        "        matrix_name = \"kv_a_layernorm\"\n",
        "        param_name = f\"model.layers.{layer_i}.self_attn.{matrix_name}.weight\"\n",
        "\n",
        "        # The rmsnorm weight is type bfloat16 and doesn't need to be dequantized.\n",
        "        # There is no bias term on the RMSNorms.\n",
        "        kv_norm_weight = retrieve_and_dequant_weight(\n",
        "            param_name,\n",
        "            \"\", # No scale name needed\n",
        "            dequant=False\n",
        "        )\n",
        "\n",
        "        # Cast to Float\n",
        "        kv_norm_weight = kv_norm_weight.float()\n",
        "\n",
        "        # kv_norm_weight is length 512\n",
        "        # input_norm_weight is length 7168\n",
        "        # W_KVA is 7168, 512\n",
        "        # W_K is 16K, 512\n",
        "        # W_V is 16K, 512\n",
        "\n",
        "        # Fold into the Key and Value projection matrices\n",
        "        W = W * kv_norm_weight.unsqueeze(0)\n",
        "\n",
        "    #\t32,768 × 512\n",
        "    W_k, W_v = torch.split(W, [num_heads*128, num_heads*128], dim=0)\n",
        "    # W_k is 16K x 512, W_v is 16k x 512\n",
        "\n",
        "    # Separate into 64 or 128 heads\n",
        "    head_ws['K'] = W_k.view(num_heads, 128, 512)\n",
        "    head_ws['V'] = W_v.view(num_heads, 128, 512)\n",
        "\n",
        "    # ======== Output ========\n",
        "    matrix_name = \"o_proj\"\n",
        "    param_name = f\"model.layers.{layer_i}.self_attn.{matrix_name}.weight\"\n",
        "    scale_name = f\"{param_name}_scale_inv\"\n",
        "\n",
        "    # Retrieve the combined output matrix, which has shape torch.Size([7168, 16384])\n",
        "    W = retrieve_and_dequant_weight(param_name, scale_name)\n",
        "\n",
        "    W_o = W\n",
        "\n",
        "    # The heads are in the columns of W_O. There are 128 heads of size 128.\n",
        "    head_ws[\"O\"] = W_o.T.reshape(num_heads, 128, 7168)\n",
        "\n",
        "    # Fuse the value and output heads\n",
        "    # W_v is\n",
        "    # head_ws[\"V\"] is a tensor with shape (num_heads, head_dim, latent_dim)\n",
        "    # W_o is\n",
        "    # head_ws[\"O\"] is a tensor with shape (num_heads, head_dim, model_dim)\n",
        "    # We want W^VO, (num_heads, latent_dim, model_dim)\n",
        "    head_ws[\"VO\"] = torch.einsum('hdc,hdm->hcm', head_ws[\"V\"], head_ws[\"O\"])\n",
        "\n",
        "    if False:\n",
        "        print(\"\\n-------- Fusion Shape --------\")\n",
        "        print(\"VOs shape:\", head_ws[\"VO\"].shape)\n",
        "        print(\"Vs shape:\", head_ws[\"V\"].shape)\n",
        "        print(\"Os shape:\", head_ws[\"O\"].shape)\n",
        "        print(\"------------------------------\\n\")\n",
        "\n",
        "    # Fuse the query and key heads.\n",
        "    # W_k is (num_heads, 128, 512)\n",
        "    # W_q is (num_heads, 128, 1536)\n",
        "    head_ws[\"KQ\"] = torch.einsum('hda,hdc->hca', head_ws['Q'], head_ws['K'])\n",
        "\n",
        "    head_ws[\"QK\"] = head_ws['KQ'].permute(0, 2, 1)\n",
        "\n",
        "    # ======== Per-Head SVD ========\n",
        "\n",
        "    print(\"\\n  Inidividual heads...\")\n",
        "\n",
        "    # Compute singular values\n",
        "    try:\n",
        "        # For each weight matrix type,\n",
        "        for W_name in ['Q', 'Q_pe', 'K', 'V', 'QK', 'KQ', 'O', 'VO']:\n",
        "\n",
        "            S_heads[layer_i][W_name] = []\n",
        "\n",
        "            # For each head in this layer,\n",
        "            for head_i in range(num_heads):\n",
        "\n",
        "                # Retrieve matrix `W_name` for head `head_i`\n",
        "                head_W = head_ws[W_name][head_i]\n",
        "\n",
        "                # Calculate its singular values\n",
        "                S_head = torch.linalg.svdvals(head_W).cpu().numpy()\n",
        "\n",
        "                # Store in our data structure.\n",
        "                S_heads[layer_i][W_name].append(S_head)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ SVD failed: {e}\")\n",
        "        continue\n",
        "\n",
        "    # ======== For Full Layer ========\n",
        "\n",
        "    print(\"\\n  Stacked heads...\")\n",
        "\n",
        "    # Finally, re-stack the heads.\n",
        "\n",
        "    # For each per-head weight matrix type,\n",
        "    for W_name in ['Q', 'Q_pe', 'K', 'V', 'QK', 'KQ', 'O', 'VO']:\n",
        "\n",
        "        # Normalize each head using the \"spectral norm\"; divide\n",
        "        # each head by its highest singular value.\n",
        "\n",
        "        # For each head in this layer,\n",
        "        for head_i in range(num_heads):\n",
        "\n",
        "            # Retrieve its largest singular value (simply the first one).\n",
        "            max_sigma = S_heads[layer_i][W_name][head_i][0]\n",
        "\n",
        "            # Divide the head's matrix by its largest sigma value.\n",
        "            head_ws[W_name][head_i] = head_ws[W_name][head_i] / max_sigma\n",
        "\n",
        "        # Combine the first two dimensions, e.g.,\n",
        "        # Reshape W^K from (num_heads, 128, 512) --> (num_heads*128, 512), or\n",
        "        # Reshape W^VO from (num_heads, 512, 7168) --> (num_heads*512, 7168).\n",
        "        W_stacked = head_ws[W_name].reshape(-1, head_ws[W_name].size(-1))\n",
        "\n",
        "        print(f\"\\n---------- {W_name} ----------\")\n",
        "        print(\"Heads:\", head_ws[W_name].shape, \" → Stacked:\", W_stacked.shape)\n",
        "\n",
        "        try:\n",
        "            # Compute the singular values for the stacked matrices of type\n",
        "            # `W_name` for the current layer `layer_i`.\n",
        "            S = torch.linalg.svdvals(W_stacked).cpu().numpy()\n",
        "\n",
        "            # Store the result in our data structure.\n",
        "            S_stacked_heads[layer_i][W_name] = S\n",
        "\n",
        "            print(f\"  Rank at 0.01% error: {get_rank_for_error_threshold(S, 0.01):,} / {len(S):,}\")\n",
        "            print(\"------------------------------\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ SVD failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(\"\\n  Subspaces...\")\n",
        "\n",
        "    # For the per-layer matrices,\n",
        "    for W_name in ['KVA', 'KVA_pe', 'QA']:\n",
        "        print(f\"\\n---------- {W_name} ----------\")\n",
        "        print(\"  Shape:\", subspace_ws[W_name].shape)\n",
        "        try:\n",
        "            # Compute the singular values for the projection matrix for this layer.\n",
        "            S = torch.linalg.svdvals(subspace_ws[W_name]).cpu().numpy()\n",
        "            S_subspaces[layer_i][W_name] = S\n",
        "\n",
        "            print(f\"  Rank at 0.01% error: {get_rank_for_error_threshold(S, 0.01):,} / {len(S):,}\")\n",
        "            print(\"------------------------------\\n\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ SVD failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # ======== Save Progress ========\n",
        "    print(\"Saving progress...\")\n",
        "\n",
        "    # At the end of each layer, save our progress to disk with pickle.\n",
        "    with open(singular_vals_file, \"wb\") as f:\n",
        "        pickle.dump((S_subspaces, S_heads, S_stacked_heads), f)\n",
        "\n",
        "\n",
        "    if backup_to_gdrive:\n",
        "        # Copy to Google Drive to back up.\n",
        "        import shutil\n",
        "        import os\n",
        "\n",
        "        # If the file already exists on google drive,\n",
        "        if os.path.exists(backup_file):\n",
        "            # Delete it\n",
        "            os.remove(backup_file)\n",
        "\n",
        "        # Copy to Google Drive to back up.\n",
        "        shutil.copy(\n",
        "            singular_vals_file,\n",
        "            backup_file\n",
        "        )\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1edtXtdPYRr"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTSgfEiiLYRb"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svbl_8gzLkZh"
      },
      "source": [
        "## Example Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjXvVCsyLoBy"
      },
      "source": [
        "For DeepSeek-R1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxoxMimkLlk3"
      },
      "source": [
        "```\n",
        "--------\n",
        "Layer 52\n",
        "--------\n",
        "\n",
        "Elapsed: 38.72 minutes\n",
        "model-00139-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]\n",
        "\n",
        "  Inidividual heads...\n",
        "\n",
        "  Stacked heads...\n",
        "\n",
        "---------- Q ----------\n",
        "Heads: torch.Size([128, 128, 1536])  → Stacked: torch.Size([16384, 1536])\n",
        "  Rank at 0.01% error: 1,277 / 1,536\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- Q_pe ----------\n",
        "Heads: torch.Size([128, 64, 1536])  → Stacked: torch.Size([8192, 1536])\n",
        "  Rank at 0.01% error: 1,108 / 1,536\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- K ----------\n",
        "Heads: torch.Size([128, 128, 512])  → Stacked: torch.Size([16384, 512])\n",
        "  Rank at 0.01% error: 497 / 512\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- V ----------\n",
        "Heads: torch.Size([128, 128, 512])  → Stacked: torch.Size([16384, 512])\n",
        "  Rank at 0.01% error: 495 / 512\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- QK ----------\n",
        "Heads: torch.Size([128, 1536, 512])  → Stacked: torch.Size([196608, 512])\n",
        "  Rank at 0.01% error: 492 / 512\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- KQ ----------\n",
        "Heads: torch.Size([128, 512, 1536])  → Stacked: torch.Size([65536, 1536])\n",
        "  Rank at 0.01% error: 1,254 / 1,536\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- O ----------\n",
        "Heads: torch.Size([128, 128, 7168])  → Stacked: torch.Size([16384, 7168])\n",
        "  Rank at 0.01% error: 6,134 / 7,168\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- VO ----------\n",
        "Heads: torch.Size([128, 512, 7168])  → Stacked: torch.Size([65536, 7168])\n",
        "  Rank at 0.01% error: 5,897 / 7,168\n",
        "------------------------------\n",
        "\n",
        "\n",
        "  Subspaces...\n",
        "\n",
        "---------- KVA ----------\n",
        "  Shape: torch.Size([512, 7168])\n",
        "  Rank at 0.01% error: 500 / 512\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- KVA_pe ----------\n",
        "  Shape: torch.Size([64, 7168])\n",
        "  Rank at 0.01% error: 64 / 64\n",
        "------------------------------\n",
        "\n",
        "\n",
        "---------- QA ----------\n",
        "  Shape: torch.Size([1536, 7168])\n",
        "  Rank at 0.01% error: 1,329 / 1,536\n",
        "------------------------------\n",
        "\n",
        "Saving progress...\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23ddec8ce2ed473cabf4893995b7fab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdbf7d7e3e7141ff94bed59821891116",
            "placeholder": "​",
            "style": "IPY_MODEL_e7a267d9cdbc49dcaa8aadd3982665eb",
            "value": "model.safetensors.index.json: "
          }
        },
        "76348540c65f4262a48a131a1fc760e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82b0b32ad31b49538c87028b7f93c00e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a33586225d874ac597709d729156b4ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8930b207d9443248cf28d77be46ae90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdbf7d7e3e7141ff94bed59821891116": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d36f208197054365bd6c4a3114905aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82b0b32ad31b49538c87028b7f93c00e",
            "placeholder": "​",
            "style": "IPY_MODEL_a33586225d874ac597709d729156b4ee",
            "value": " 8.90M/? [00:00&lt;00:00, 23.9MB/s]"
          }
        },
        "e61f4a720b81411597f1066c68ec9d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23ddec8ce2ed473cabf4893995b7fab2",
              "IPY_MODEL_fb8d73df90304542b2cb4e850ba58f0f",
              "IPY_MODEL_d36f208197054365bd6c4a3114905aad"
            ],
            "layout": "IPY_MODEL_76348540c65f4262a48a131a1fc760e2"
          }
        },
        "e7a267d9cdbc49dcaa8aadd3982665eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1010654c5944e938b59f70ea330fdc5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fb8d73df90304542b2cb4e850ba58f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1010654c5944e938b59f70ea330fdc5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8930b207d9443248cf28d77be46ae90",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}